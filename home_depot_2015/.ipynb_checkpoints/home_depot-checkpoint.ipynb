{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Home Depot Product Search Relevance\n",
      "The goal of this analysis is to determine how to predict relevance of a search on Home Depot's website. The training data were labelled by crowdsourcing humans, but the hope is that the text and numerical features will be enough to predict relevance via machine learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import my library stack\n",
      "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
      "import os\n",
      "import pprint\n",
      "import copy\n",
      "import gc\n",
      "import re\n",
      "%matplotlib inline\n",
      "\n",
      "# Some nice display tools for ipython\n",
      "from IPython.display import display, HTML\n",
      "\n",
      "# There are several files that the Kaggle competition included for this analysis\n",
      "DATADIR = \"%s/home_depot_2015/\"%os.environ[\"KAGGLE_DATA_DIR\"] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1: Overview of the data\n",
      "There are three files I will take a peek at here:\n",
      "    \n",
      "* **train.csv** -- The training set, which contains products, searches, and relevance scores\n",
      "* **test.csv** -- The test set, which contains products and searches --> I am to predict relevance scores\n",
      "    \n",
      "* **product_descriptions.csv** -- Contains product id and a plain text description of the product\n",
      "    \n",
      "* **attributes.csv** -- Contains product id and several attributes, but for only a *subset* of products\n",
      "\n",
      "I'm just going to preview the first few rows of each."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preview the first 5 rows of a csv file given the path to it\n",
      "def preview_data(file, name):\n",
      "    print(\"shit\")\n",
      "    print( display(HTML(\"<h3>First three rows of %s</h3>\"%name)) )\n",
      "    preview_df = pd.read_csv(file, encoding=\"ISO-8859-1\")\n",
      "    print( display(preview_df.head(1)) )\n",
      "\n",
      "def get_path(file):\n",
      "    return \"%s%s\"%(DATADIR, file)\n",
      "\n",
      "# Define the files for later\n",
      "f_train = get_path('train.csv')\n",
      "f_test = get_path('test.csv')\n",
      "f_desc = get_path('product_descriptions.csv')\n",
      "f_attr = get_path('attributes.csv')\n",
      "\n",
      "# Do all four\n",
      "files = [(f_train, 'train'), (f_test, 'test'), (f_desc, 'descriptions'), (f_attr, 'attributes')]\n",
      "map(lambda x: preview_data(x[0], x[1]), files)\n",
      "print(\"hello\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hello\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2: Feature engineering\n",
      "After looking at these spreadsheets, I realize there isn't a ton of information with which to work. My initial thought is to do some sort of a word matching procedure (e.g. see if one of the search words matches one of the words in the title (or description, or attributes). Better still, I could take the individual letters in each word of the search query and try to see if they appear consecutively in the raw string of the title, description, or attributes.\n",
      "\n",
      "Let's give that a try."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I will definitely want to use multiprocessing in the coming steps\n",
      "from multiprocessing import Pool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Custom manipulation of attributes\n",
      "The attributes file has a dump of attributes and their respective product_uid values. I want two things out of this file:\n",
      "* A concatenation of all the \"values\"; that is, all of the raw text information\n",
      "* Specifically the brand name (marked as \"MFG Brand Name\")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The attributes are a little trickier. There may be 0 or many attributes per 1 product_uid\n",
      "# I want to concatenate all strings belonging to a particular product_uid\n",
      "\n",
      "# Data[0] = product_uid; data[1] = name; data[2] = value\n",
      "def collapse_attr(data):\n",
      "    attr = {}\n",
      "    for d in data:\n",
      "        # d is an array of form [product_uid, name, value]\n",
      "        if not np.isnan(d[0]):\n",
      "            i = str(int(d[0]))\n",
      "            # Concatenate the attribute as a string\n",
      "            # Also add the brand if it exists\n",
      "            if i in attr:\n",
      "                attr[i]['string'] = \"%s %s\"%( attr[i]['string'], str(d[2]) )\n",
      "                if d[1] == 'MFG Brand Name':\n",
      "                    attr[i]['brand'] = str(d[2])\n",
      "            else:\n",
      "                attr[i] = {'string': str(d[2])}\n",
      "                \n",
      "                if d[1] == 'MFG Brand Name':\n",
      "                    attr[i]['brand'] = str(d[2])\n",
      "                else:\n",
      "                    attr[i]['brand'] = ''\n",
      "\n",
      "    return attr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the training attribute array, which we will append to the dataframe in the pipeline\n",
      "attr = pd.read_csv(f_attr)\n",
      "ATTR_ARR = collapse_attr(np.array(attr))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(ATTR_ARR['100001'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'brand': 'Simpson Strong-Tie', 'string': 'Versatile connector for various 90\u00b0 connections and home repair projects Stronger than angled nailing or screw fastening alone Help ensure joints are consistently straight and strong Dimensions: 3 in. x 3 in. x 1-1/2 in. Made from 12-Gauge steel Galvanized for extra corrosion resistance Install with 10d common nails or #9 x 1-1/2 in. Strong-Drive SD screws 12 Galvanized Steel Simpson Strong-Tie 1 1.5 3 0.26 3'}\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Custom functions for processing the strings\n",
      "These will format the strings, add alternate suffixes, and add some common abbreviations if applicable. Since we're dealing with Home Depot data, we have a general idea of what types of abbreviations we might encounter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Given a word, return all forms of it and its abbreviations\n",
      "def abbrev(s):\n",
      "    abrv_groups = [\n",
      "        [\"'\", \"in\", \"inches\", \"inch\"],\n",
      "        [\"pounds\", \"pound\", \"lbs\", \"lb\"],\n",
      "        [\"sqft\", \"sq\", \"sf\", \"square\", \"squared\", \"foot\", \"feet\", \"\\\"\", \"ft\", \"inch\", \"inches\", \"in\", \"'\"],\n",
      "        [\"cf\", \"cu\", \"cubic\", \"cubed\", \"inch\", \"foot\", \"feet\", \"'\", \"\\\"\", \"ft\", \"in\", \"inches\"],\n",
      "        [\"gal\", \"gallon\", \"gallons\"],\n",
      "        [\"g\", \"gram\", \"grams\", \"kg\", \"kilogram\", \"kilo\"],\n",
      "        [\"oz\", \"ounces\", \"ounce\"],\n",
      "        [\"cm\", \"centimeters\", \"centimeter\"],\n",
      "        [\"m\", \"meter\", \"meters\"],\n",
      "        [\"mm\", \"milimeter\", \"millimeter\", \"milimeters\", \"millimeters\"],\n",
      "        [\"a\", \"amp\", \"amps\", \"ampere\", \"amperes\"],\n",
      "        [\"w\", \"watt\", \"watts\"],\n",
      "        [\"v\", \"volt\", \"volts\"],\n",
      "        [\"whirpool\",\"whirlpool\", \"whirlpoolga\", \"whirlpoolstainless\",\"stainless\"],\n",
      "        [\"and\", \"&\", \"+\", \"&amp;\"],\n",
      "        [\"x\", \"by\", \"*\"],\n",
      "        [\"deg\", \"degree\", \"degrees\", \"\u00b0\", \"angle\"],\n",
      "        ['dia', 'diameter']\n",
      "    ]\n",
      "    \n",
      "    # If we can match the word in an abbreviation group, return the whole group\n",
      "    for g in abrv_groups:\n",
      "        if s in g:\n",
      "            return g\n",
      "        \n",
      "    # If we can't match anything just return an empty array\n",
      "    return []\n",
      "\n",
      "# Turn the string into a series of words\n",
      "def process_string(s):\n",
      "    \n",
      "    __words = np.array(s.split(\" \"))\n",
      "    \n",
      "    # Split by special, but include those characters\n",
      "    # This regex splits by the characters [', \", /, *, -], but INCLUDES those characters\n",
      "    _words = list(np.hstack(map(lambda i: re.split(r\"(plus|[('\\\"\\-*/)])\", i), __words)))\n",
      "\n",
      "    words = _words\n",
      "    \n",
      "    # Get rid of commas\n",
      "    words = map(lambda x: x.replace(',', ''), words)\n",
      "    # Get rid of semicolons\n",
      "    words = map(lambda x: x.replace(';', ''), words)\n",
      "    # Get rid of colons\n",
      "    words = map(lambda x: x.replace(':', ''), words)\n",
      "    # Get rid of periods\n",
      "    words = map(lambda x: x.replace('.', ''), words)\n",
      "    # Get rid of blanks\n",
      "    words = filter(lambda x: x != ' ' and x != '', words)\n",
      "    return words\n",
      "\n",
      "\n",
      "def pre_process_strings(query):\n",
      "    \n",
      "    # Lowercase all the things\n",
      "    query = str(query).lower()\n",
      "    \n",
      "    # Split the query into an array of char arrays\n",
      "    query_words = process_string(query)\n",
      "\n",
      "    return query_words\n",
      "\n",
      "\n",
      "# This function processes strings like \"4x4\" or \"4'x4'\"\n",
      "def process_x_by(s):\n",
      "    if any(i.isdigit() for i in s) and \"x\" in s:\n",
      "        new_s = list(filter(lambda x: x!='', s.split(\"x\"))); new_s.append(s); new_s.append(\"x\")\n",
      "        return new_s\n",
      "    else:\n",
      "        return [s]\n",
      "\n",
      "# Split strings that have unit suffixes (e.g. 10in, 50g, etc)\n",
      "def process_unit_suffixes(s):\n",
      "    strings = [\"mm\", \"cm\", \"m\", \"g\", \"kg\", \"in\", \"ft\", \"a\", \"w\", \"v\", \"oz\", \"gal\", \"cf\"]\n",
      "    # Inefficient but I can't think of a better way\n",
      "    for i in strings:\n",
      "        # If any of the above strings is in s, return that string + the number\n",
      "        if i in s and any(j.isdigit() for j in s):\n",
      "            arr = list(filter(lambda x: x!='', s.split(i))); arr.append(i); arr.append(s)\n",
      "            return arr\n",
      "\n",
      "    return [s]\n",
      "\n",
      "# Get a list of words similar to the word if applicable\n",
      "# This will get called with a word in the QUERY\n",
      "def extension_words(word):\n",
      "    \n",
      "    # Make damn sure everything is lower case\n",
      "    w = word.lower()\n",
      "    \n",
      "    # Process strings of the form \"AxB\"\n",
      "    ret_words = process_x_by(w)\n",
      "    \n",
      "    # Include abbreviation words\n",
      "    abbr = abbrev(w)\n",
      "    \n",
      "    # Flatten array\n",
      "    ret_words = list(np.hstack([ret_words, abbr]))\n",
      "    \n",
      "    \n",
      "    # If the word is small (<4 chars), contains a number, or contains a special character,\n",
      "    #     only add s and return\n",
      "    if any(i.isdigit() for i in w) or len(list(word)) < 4 or any(i in w for i in [\"-\", \"*\", \"'\", \"\\\"\", \"/\"]):\n",
      "        ret_words.append(\"%ss\"%w)\n",
      "        return filter(lambda x: x!='' and x!=' ', ret_words)\n",
      "\n",
      "    \n",
      "    # A list of suffixes\n",
      "    suffixes = ['s', 'ed', 'ing', 'n', 'en', 'er', 'est', 'ise', 'fy', 'ly',\n",
      "               'ful', 'able', 'ible', 'hood', 'ess', 'ness', 'less', 'ism',\n",
      "               'ment', 'ist', 'al', 'ish', 'tion']\n",
      "    \n",
      "    # If the word ends in one of these suffixes, add the smaller version\n",
      "    # to strings; otherwise, add this to the end of the word and add that\n",
      "    for x in range(len(suffixes)):\n",
      "        l = len(suffixes[x])\n",
      "        if w[-l:] == suffixes[x]:\n",
      "            ret_words.append(w[0:-l])\n",
      "        else:\n",
      "            ret_words.append(w+suffixes[x])\n",
      "\n",
      "    return filter(lambda x: x!='' and x!=' ', ret_words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Custom functions for doing word searches\n",
      "These will determine if words in the query are in the matching string. We want to know three basic things:\n",
      "* Does the string contain *any* of the query words?\n",
      "* What fraction of the query words are in the matching words?\n",
      "* What fraction of the chars making up query words are found in the matching words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Determine if the word is in the comparison string\n",
      "#   @returns 1 or 0\n",
      "def match_word(word, compare):\n",
      "    strings = extension_words(word)\n",
      "    if not strings: return 0\n",
      "    return min(1, sum( map(lambda s: 1 if s in compare else 0, strings ) ))\n",
      "\n",
      "# Determine the number of times the word (or any version of it) matches a string\n",
      "#   @returns array of match counts\n",
      "def match_word_count(word, compare):  \n",
      "    strings = filter( lambda x: x!='' and x!=' ', extension_words(word) )\n",
      "    if not strings: return 0\n",
      "    return max( map(lambda s: compare.count(s) , strings ) )\n",
      "    \n",
      "    \n",
      "    \n",
      "## STRING MATCHING\n",
      "##=========================================================\n",
      "\n",
      "# Get the number of unique words that are matched\n",
      "def matched_words_string(query, to_match):\n",
      "    query_words = pre_process_strings(query)\n",
      "    return sum( map(lambda x: match_word(x, to_match.lower()), query_words) ) if query_words else 0\n",
      "    \n",
      "    \n",
      "# Get the count of all words matched (i.e. if a word is matched more than once, it is counted multiple times)\n",
      "def count_matched_words_string(query, to_match):\n",
      "    query_words = pre_process_strings(query)\n",
      "    return sum( map(lambda x: match_word_count(x, to_match.lower()), query_words) )\n",
      "\n",
      "## Word matching to the words in the string\n",
      "def matched_words_words(query, to_match):\n",
      "    query_words = pre_process_strings(query)\n",
      "    to_match_words = to_match.split(\" \")\n",
      "    sums = map(lambda y: sum( map(lambda x: match_word(x, to_match.lower()), query_words) ), \\\n",
      "                to_match_words) if query_words else 0\n",
      "    return sum(sums)\n",
      "\n",
      "# Count of words matching the words in the string\n",
      "def count_matched_words_words(query, to_match):\n",
      "    query_words = pre_process_strings(query)\n",
      "    to_match_words = to_match.split(\" \")\n",
      "    counts = map(lambda y: sum( map(lambda x: match_word_count(x, to_match.lower()), query_words) ), \\\n",
      "                to_match_words) if query_words else 0\n",
      "    return sum(counts)\n",
      "        \n",
      "\n",
      "\n",
      "## QUERY MATCHING\n",
      "##=========================================================\n",
      "\n",
      "# Whether or not the whole query is in the string\n",
      "def matched_query(query, to_match):\n",
      "    return query in to_match\n",
      "\n",
      "# How many times the whole query is in the string\n",
      "def count_matched_query(query, to_match):\n",
      "    return to_match.count(query)\n",
      "\n",
      "# Whether or not the last word of the query is in the to_match string\n",
      "def last_word(query, to_match):\n",
      "    last_q = query.split(\" \")[-1]\n",
      "    return 1 if last_q in to_match else 0\n",
      "\n",
      "# Whether or not a word from the query is the FIRST word in the to_match string\n",
      "def first_word(query, to_match):\n",
      "    first_q = query.split(\" \")[0]\n",
      "    return 1 if first_q in to_match else 0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3: Feature Engineering Pipeline\n",
      "I will start by engineering new features and removing the long strings in my data set. Specifically, I want to add\n",
      "\n",
      "* Match rates of query relating to title and description (determined by char_match_fraction function)\n",
      "* String length columns of query, description, and title columns\n",
      "\n",
      "I will go ahead and build a new training set based on this."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Lambda functions and Pool\n",
      "This is a lot of extra code but multiprocessing will speed things up significantly and the lambda functions have to be defined in scope."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# POOL lambda functions (need to be defined outside the function that calls them)\n",
      "# With multiprocessing, we can't use lambda, so I will define some basic functions here\n",
      "#================================================================================\n",
      "\n",
      "# Is the product_uid a key in the attribute array?\n",
      "def lambda_in_attr(a):\n",
      "    a = str(int(a))\n",
      "    return ATTR_ARR[a]['string'] if a in ATTR_ARR else ''\n",
      "def lambda_brand(a):\n",
      "    a = str(int(a))\n",
      "    return ATTR_ARR[a]['brand'] if a in ATTR_ARR else ''\n",
      "\n",
      "\n",
      "# Lengths\n",
      "def lambda_char_len(a):\n",
      "    return float(len(list(str(a))))\n",
      "def lambda_word_len(a):\n",
      "    return float(len(str(a).split(\" \")))\n",
      "\n",
      "\n",
      "# Matched words\n",
      "def l_matched_words_string(a):\n",
      "    return float(matched_words_string( str(a[0]), str(a[1]) ))\n",
      "    \n",
      "def l_count_matched_words_string(a):\n",
      "    return float(count_matched_words_string( str(a[0]), str(a[1]) ))\n",
      "def l_matched_words_words(a):\n",
      "    return float(matched_words_words( str(a[0]), str(a[1]) ))\n",
      "def l_count_matched_words_words(a):\n",
      "    return float(count_matched_words_words( str(a[0]), str(a[1]) ))\n",
      "\n",
      "\n",
      "# Matched queries\n",
      "def l_matched_query(a):\n",
      "    return matched_query( str(a[0]), str(a[1]) )\n",
      "def l_count_matched_query(a):\n",
      "    return float(count_matched_query( str(a[0]), str(a[1]) ))\n",
      "\n",
      "\n",
      "# Binaries\n",
      "def l_last_word(a):\n",
      "    return last_word( str(a[0]), str(a[1]) )\n",
      "def l_first_word(a):\n",
      "    return first_word( str(a[0]), str(a[1]) )\n",
      "\n",
      "# Define the pool in scope\n",
      "# An abstracted multiprocessor functional so that I can remove MP when debugging\n",
      "# All functions are applied to a map\n",
      "POOL = Pool(maxtasksperchild=1000)\n",
      "\n",
      "\n",
      "def run_pool(f, iterator):\n",
      "    #m = map(f, iterator)\n",
      "    m = POOL.map(f, iterator)\n",
      "    return pd.Series(m)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Feature pipeline\n",
      "Here I will add all of my features to the dataframe that will be used as X_train"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import sys\n",
      "\n",
      "\n",
      "#================================================================================\n",
      "## DATA PIPELINE\n",
      "#================================================================================\n",
      "# Given the data (train or test) and description files,\n",
      "# perform a series of operations to produce a data set on which we can do ML\n",
      "def feature_pipeline(data_file, **kwargs):\n",
      "    \n",
      "    # Define my multiprocessing pool and start the timer\n",
      "    \n",
      "    start = time.time()\n",
      "    \n",
      "    \n",
      "    #============\n",
      "    # Read files\n",
      "    #============\n",
      "    \n",
      "    # Read the initial train.csv and join it to product descriptions\n",
      "    _df = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
      "    #_df = pd.read_csv(data_file)\n",
      "\n",
      "    # Add in descriptions because they are 1:1\n",
      "    df = pd.merge(_df, pd.read_csv(f_desc), how='outer')\n",
      "    \n",
      "    # If there is an attribute for a product uid, join it\n",
      "    df['attr'] = run_pool(lambda_in_attr, df['product_uid'])\n",
      "    df['brand'] = run_pool(lambda_brand, df['product_uid'])\n",
      "    \n",
      "    \n",
      "    # Construct the columns to be added to the dataframe. Each will be added with run_pool defined above.\n",
      "    \n",
      "    # Char lengths\n",
      "    char_lengths = [\n",
      "        ['desc_char_l', lambda_char_len, df['product_description'] ],\n",
      "        ['title_char_l', lambda_char_len, df['product_title'] ],\n",
      "        ['query_char_l', lambda_char_len, df['search_term'] ],\n",
      "        ['attr_char_l', lambda_char_len, df['attr'] ],\n",
      "        ['brand_char_l', lambda_char_len, df['brand'] ]\n",
      "    ]\n",
      "\n",
      "    \n",
      "    # Word lengths\n",
      "    word_lengths = [\n",
      "        ['desc_word_l', lambda_word_len, df['product_description']],\n",
      "        ['title_word_l', lambda_word_len, df['product_title']],\n",
      "        ['query_word_l', lambda_word_len, df['search_term']],\n",
      "        ['attr_word_l', lambda_word_len, df['attr']],\n",
      "        ['brand_word_l', lambda_word_len, df['brand']]\n",
      "    ]\n",
      "\n",
      "    \n",
      "    # Zip the data into tuples\n",
      "    desc_zip = np.dstack( ( np.array(df['search_term']), np.array(df['product_description']) ))[0]\n",
      "    title_zip = np.dstack( (np.array(df['search_term']), np.array(df['product_title']) ))[0]\n",
      "    attr_zip = np.dstack( (np.array(df['search_term']), np.array(df['attr']) ))[0]\n",
      "    brand_zip = np.dstack( (np.array(df['search_term']), np.array(df['brand']) ))[0]\n",
      "\n",
      "    \n",
      "    # Number of unique words that are matched to the comparison string (float)\n",
      "    matched_strings = [\n",
      "        ['desc_matched_string', l_matched_words_string, desc_zip],\n",
      "        ['title_matched_string', l_matched_words_string, title_zip],\n",
      "        ['attr_matched_string', l_matched_words_string, attr_zip],\n",
      "        ['brand_matched_string', l_matched_words_string, brand_zip]\n",
      "    ]\n",
      "    \n",
      "    \n",
      "    # Total number of query words matched to the comparison string (float)\n",
      "    count_string = [\n",
      "        ['desc_count_string', l_count_matched_words_string, desc_zip],\n",
      "        ['title_count_string', l_count_matched_words_string, title_zip],\n",
      "        ['attr_count_string', l_count_matched_words_string, attr_zip]\n",
      "    ]\n",
      "\n",
      "    \n",
      "    # Whether or not the whole query (string) can be found in the comparison string (bool)\n",
      "    query_matched = [\n",
      "        ['desc_query_matched', l_matched_query, desc_zip],\n",
      "        ['title_query_matched', l_matched_query, title_zip],\n",
      "        ['attr_query_matched', l_matched_query, attr_zip]\n",
      "    ]\n",
      "    \n",
      "    \n",
      "    # How many times the query is in the comparison string (float)\n",
      "    query_count = [\n",
      "        ['desc_query_count', l_count_matched_query, desc_zip],\n",
      "        ['title_query_count', l_count_matched_query, title_zip],\n",
      "        ['attr_query_count', l_count_matched_query, attr_zip]\n",
      "    ]\n",
      "    \n",
      "\n",
      "    # Is the last word of the query in the comparison string? (bool)\n",
      "    last_word = [\n",
      "        ['desc_last_word', l_last_word, desc_zip],\n",
      "        ['title_last_word', l_last_word, title_zip]\n",
      "    ]\n",
      "    \n",
      "    # Is the first word of the query in the comparison string? (bool)\n",
      "    first_word = [\n",
      "        ['desc_first_word', l_first_word, desc_zip],\n",
      "        ['title_first_word', l_first_word, title_zip]\n",
      "    ]\n",
      "    \n",
      "\n",
      "    ## Add the columns\n",
      "    cols_to_add = [\n",
      "                   char_lengths, \n",
      "                   word_lengths, \n",
      "                   matched_strings, \n",
      "                   count_string, \n",
      "                   query_matched, \n",
      "                   query_count, \n",
      "                   last_word, \n",
      "                   first_word\n",
      "                   ]\n",
      "    for cols in cols_to_add:\n",
      "        for col in cols:\n",
      "            df[col[0]] = run_pool(col[1], col[2])\n",
      "    \n",
      "    # Fraction of unique words matched divided by number of unique words in the query (float)\n",
      "    df['desc_frac_matched'] = df['desc_matched_string'] / df['query_word_l']\n",
      "    df['title_frac_matched'] = df['title_matched_string'] / df['query_word_l']\n",
      "    df['attr_frac_matched_query'] = df['attr_matched_string'] / df['query_word_l']\n",
      "    # Also by the attr length\n",
      "    df['attr_frac_matched_attr'] = df['attr_matched_string'] / df['attr_word_l']\n",
      "    \n",
      "    \n",
      "    # Drop NaNs\n",
      "    copy_df = copy.deepcopy(df.dropna())\n",
      "    \n",
      "    print( display(HTML(\"<font color='blue'><b>Data pipelined in %s s</b></font>\"%(time.time()-start))) )\n",
      "    return copy_df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = feature_pipeline(f_train)\n",
      "y_train = X_train['relevance'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<font color='blue'><b>Data pipelined in 41.56711506843567 s</b></font>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x7fd10d3d8390>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 4: Plot the Feature Distributions\n",
      "As a sanity check, it is good to check out the first few lines of my data frame and also to graph the features to make sure there are actual distributions of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the distribution (histogram) of my features\n",
      "def plot_hist(col, name):\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    \n",
      "    plt.title('%s' %name, fontsize=15)\n",
      "    #fig.colorbar(cax)\n",
      "    plt.hist(col)\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "def run_feature_plots(df):\n",
      "    # Feature plots\n",
      "    skip_cols = ['id', 'product_uid', 'relevance','search_term', 'brand',\n",
      "                     'product_title','product_description','attr']\n",
      "    \n",
      "    features = list(df.drop(skip_cols, axis=1).columns.values)\n",
      "    # Plot a bunch of stuff\n",
      "    dim = len(features)/3 + 1 if len(features)%3 > 0 else len(features)/3\n",
      "\n",
      "    f, axarr = plt.subplots(dim, 3, figsize=(16,20))\n",
      "    plt.tight_layout(pad=3)\n",
      "    \n",
      "    for i in range(0, dim):\n",
      "        # For each row\n",
      "        for j in range(0, 3):\n",
      "            # For each element in the row\n",
      "            if (i*3 + j) < len(features):\n",
      "                # As long as the chart exists in the tuple\n",
      "                axarr[i][j].hist( df[ features[i*3+j] ], color='orange' )\n",
      "                axarr[i][j].set_title( features[i*3+j], fontsize=15 )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#run_feature_plots(X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 5: Learning\n",
      "A few notes about the distributions:\n",
      "\n",
      "* The string length columns look to be distributed pretty nicely\n",
      "* The description matches are heavily favored to the right (meaning the strings match well); we would expect this from a search engine\n",
      "* The relevance scores are also heavily favored to the right (again, we expect this engine to work reasonably well, so this makes makes sense)\n",
      "\n",
      "Everything so far looks reasonable. Now I will go ahead and set up a machine learning pipeline to test some algorithms on the training/test data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Pipeline Transformers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "\n",
      "# Define my custom pipeline\n",
      "class CustomPipeline(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def fit(self, x, y=None):\n",
      "        return self\n",
      "    \n",
      "    def transform(self, df):\n",
      "        drop_cols = ['id', 'product_uid', 'relevance','search_term', 'brand',\n",
      "                     'product_title','product_description','attr']\n",
      "        new_df = df.drop(drop_cols, axis=1).values\n",
      "        return new_df\n",
      "    \n",
      "# This is a \n",
      "class TextPipeline(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def __init__(self, key):\n",
      "        self.key = key\n",
      "    \n",
      "    def fit(self, x, y=None):\n",
      "        return self\n",
      "    \n",
      "    def transform(self, data_dict):\n",
      "        # Convert dict into a string\n",
      "        return data_dict[self.key].apply(str)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor as BR\n",
      "from sklearn import pipeline, grid_search\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "\n",
      "## Use a random forest\n",
      "rfr = RandomForestRegressor(n_jobs=-1, n_estimators=400, max_depth=15)\n",
      "\n",
      "# Use tf-idf sk-learn functions to vectorize the documents\n",
      "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
      "# After tf-idf, reduce dimensionality of the vector\n",
      "tsvd = TruncatedSVD(n_components=10)\n",
      "\n",
      "## Define the pipeline\n",
      "_pipeline = pipeline.Pipeline([\n",
      "    ('union', FeatureUnion(\n",
      "        transformer_list = [\n",
      "            ('cst',  CustomPipeline()),\n",
      "            ('txt1', pipeline.Pipeline([\n",
      "                ('s1', TextPipeline(key='search_term')),\n",
      "                ('tfidf1', tfidf), \n",
      "                ('tsvd1', tsvd)\n",
      "            ])),\n",
      "            ('txt2', pipeline.Pipeline([\n",
      "                ('s2', TextPipeline(key='product_title')), \n",
      "                ('tfidf2', tfidf),\n",
      "                ('tsvd2', tsvd)\n",
      "            ])),\n",
      "            ('txt3', pipeline.Pipeline([\n",
      "                ('s3', TextPipeline(key='product_description')), \n",
      "                ('tfidf3', tfidf), \n",
      "                ('tsvd3', tsvd)])\n",
      "            ),\n",
      "            ('txt4', pipeline.Pipeline([\n",
      "                ('s4', TextPipeline(key='brand')),\n",
      "                ('tfidf4', tfidf), \n",
      "                ('tsvd4', tsvd)\n",
      "            ]))        \n",
      "        ],\n",
      "        transformer_weights = {\n",
      "            'cst': 1.0,\n",
      "            'txt1': 0.5,\n",
      "            'txt2': 0.0,\n",
      "            'txt3': 0.0,\n",
      "            'txt4': 0.5\n",
      "        },\n",
      "        n_jobs = -1\n",
      "    )), \n",
      "    ('rfr', rfr)])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_squared_error, make_scorer\n",
      "\n",
      "# Define the loss function; this is a custom root-MSE (RMSE) function with tighter errors\n",
      "def f_mse(y, y_pred):\n",
      "    return mean_squared_error(y, y_pred)**0.5\n",
      "RMSE = make_scorer(f_mse, greater_is_better=False)\n",
      "\n",
      "# Param grid for GridSearch\n",
      "param_grid = { 'rfr__max_features': [10],'rfr__max_depth': [20] }\n",
      "\n",
      "# Arguments for GridSearch\n",
      "grid_search_args = {\n",
      "    'estimator': _pipeline,\n",
      "    'param_grid': param_grid,\n",
      "    'n_jobs': -1,\n",
      "    'cv': 4,\n",
      "    'verbose': 0,\n",
      "    'scoring': RMSE\n",
      "}\n",
      "\n",
      "# Define the model\n",
      "model = grid_search.GridSearchCV(**grid_search_args)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# RUN GridSearch\n",
      "start = time.time()\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "print(\"GridSearchCV completed in %s s\"%(time.time()-start))\n",
      "print(\"Best parameters found by grid search: %s\"%model.best_params_)\n",
      "print(\"Best CV score: %s\"%model.best_score_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:497: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py:523: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
        "  for name, trans in self.transformer_list)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GridSearchCV completed in 108.75439476966858 s\n",
        "Best parameters found by grid search: {'rfr__max_depth': 20, 'rfr__max_features': 10}\n",
        "Best CV score: -0.471046717815\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 6: Test Set\n",
      "Now I will move over to the test set. I will predict based on the model I just generated."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6.1 Pipeline Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_test = feature_pipeline(f_test)\n",
      "\n",
      "# Separate the ids\n",
      "df_test_ids = df_test['id']\n",
      "\n",
      "# Need to add this column temporarily; it will get dropped in the pipeline\n",
      "df_test['relevance'] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<font color='blue'><b>Data pipelined in 64.069020986557 s</b></font>"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.HTML at 0x7f8b1e906b70>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "None\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6.2 Predict test_y"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Predict!\n",
      "test_y = model.predict(df_test)\n",
      "final_y = map(lambda x: 1 if x < 1. else 3 if x > 3. else x, test_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n",
        "/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py:2645: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 6.3 Look at distributions\n",
      "After looking at the data, I want to look at the distribution and compare it to the one from the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_hist(final_y, 'Scores in Test Set')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "RuntimeError",
       "evalue": "matplotlib does not support generators as input",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-24-e4f87af85f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Scores in Test Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-12-e697a4cfe9fc>\u001b[0m in \u001b[0;36mplot_hist\u001b[0;34m(col, name)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#fig.colorbar(cax)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   2956\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1811\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1812\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[1;32m   5958\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5959\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5960\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5961\u001b[0m         \u001b[0mnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# number of datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_normalize_input\u001b[0;34m(inp, ename)\u001b[0m\n\u001b[1;32m   5889\u001b[0m             \"\"\"\n\u001b[1;32m   5890\u001b[0m             if (isinstance(x, np.ndarray) or\n\u001b[0;32m-> 5891\u001b[0;31m                     not iterable(cbook.safe_first_element(inp))):\n\u001b[0m\u001b[1;32m   5892\u001b[0m                 \u001b[0;31m# TODO: support masked arrays;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5893\u001b[0m                 \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36msafe_first_element\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_first_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m         raise RuntimeError(\"matplotlib does not support generators \"\n\u001b[0m\u001b[1;32m   2543\u001b[0m                            \"as input\")\n\u001b[1;32m   2544\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mRuntimeError\u001b[0m: matplotlib does not support generators as input"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEMCAYAAAAh7MZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwGL/D7JKTfOKRDGRAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f8ae8e41748>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_hist(train_y, 'Normalized Relevance')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Test mean: %s, std: %s\"%(np.mean(final_y), np.std(final_y)))\n",
      "print(\"Training mean: %s, std: %s\"%(np.mean(train_y), np.std(train_y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "unsupported operand type(s) for /: 'map' and 'int'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-26-5642928f4643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test mean: %s, std: %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training mean: %s, std: %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2878\u001b[0;31m                           out=out, keepdims=keepdims)\n\u001b[0m\u001b[1;32m   2879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'map' and 'int'"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 7: Submission\n",
      "Now I am finally ready to write the submission file!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_name = \"submission4\"\n",
      "\n",
      "path = \"%s/%s.csv\"%(DATADIR, file_name)\n",
      "pd.DataFrame({\"id\": df_test_ids.apply(int), \"relevance\": test_y}).to_csv(path, index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}