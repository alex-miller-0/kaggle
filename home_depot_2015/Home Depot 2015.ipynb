{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Product Search Relevance\n",
    "The goal of this analysis is to determine how to predict relevance of a search on Home Depot's website. The training data were labelled by crowdsourcing humans, but the hope is that the text and numerical features will be enough to predict relevance via machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import my library stack\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "%matplotlib inline\n",
    "\n",
    "# Some nice display tools for ipython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# There are several files that the Kaggle competition included for this analysis\n",
    "DATADIR = \"%s/home_depot_2015/\"%os.environ[\"KAGGLE_DATA_DIR\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of the data\n",
    "There are three files I will take a peek at here:\n",
    "    \n",
    "* **train.csv** -- The training set, which contains products, searches, and relevance scores\n",
    "* **test.csv** -- The test set, which contains products and searches --> I am to predict relevance scores\n",
    "    \n",
    "* **product_descriptions.csv** -- Contains product id and a plain text description of the product\n",
    "    \n",
    "* **attributes.csv** -- Contains product id and several attributes, but for only a *subset* of products\n",
    "\n",
    "I'm just going to preview the first few rows of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview the first 5 rows of a csv file given the path to it\n",
    "def preview_data(file, name):\n",
    "    print display(HTML(\"<br><h2>First five rows of %s</h2>\"%name))\n",
    "    preview_df = pd.read_csv(file)\n",
    "    print display(preview_df.head(5))\n",
    "\n",
    "def get_path(file):\n",
    "    return \"%s%s\"%(DATADIR, file)\n",
    "\n",
    "# Define the files for later\n",
    "f_train = get_path('train.csv')\n",
    "f_test = get_path('test.csv')\n",
    "f_desc = get_path('product_descriptions.csv')\n",
    "f_attr = get_path('attributes.csv')\n",
    "\n",
    "# Do all four\n",
    "files = [(f_train, 'train'), (f_test, 'test'), (f_desc, 'descriptions'), (f_attr, 'attributes')]\n",
    "map(lambda x: preview_data(x[0], x[1]), files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering: Part 1\n",
    "After looking at these spreadsheets, I realize there isn't a ton of information with which to work. My initial thought is to do some sort of a word matching procedure (e.g. see if one of the search words matches one of the words in the title (or description, or attributes). Better still, I could take the individual letters in each word of the search query and try to see if they appear consecutively in the raw string of the title, description, or attributes.\n",
    "\n",
    "Let's give that a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I will definitely want to use multiprocessing in this one\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The attributes are a little trickier. There may be 0 or many attributes per 1 product_uid\n",
    "# I want to concatenate all strings belonging to a particular product_uid\n",
    "def collapse_attr(data):\n",
    "    attr = {}\n",
    "    for d in data:\n",
    "        # d is an array of form [product_uid, name, value]\n",
    "        if not np.isnan(d[0]):\n",
    "            # Concatenate the attribute if it exists, otherwise add it\n",
    "            if str(int(d[0])) in attr:\n",
    "                attr[str(int(d[0]))] = \"%s %s\"%( attr[str(int(d[0]))], str(d[2]) )\n",
    "            else:\n",
    "                attr[str(int(d[0]))] = str(d[2])\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training attribute array, which we will append to the dataframe in the pipeline\n",
    "ATTR_ARR = collapse_attr(np.array(pd.read_csv(f_attr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for processing the strings\n",
    "These will format the strings, add alternate suffixes, and add some common abbreviations if applicable. Since we're dealing with Home Depot data, we have a general idea of what types of abbreviations we might encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given a word, return all forms of it and its abbreviations\n",
    "def abbrev(s):\n",
    "    abrv_groups = [\n",
    "        [\"'\", \"in\", \"inches\", \"inch\"],\n",
    "        [\"pounds\", \"pound\", \"lbs\", \"lb\"],\n",
    "        [\"sqft\", \"sq\", \"square\", \"foot\", \"feet\", \"\\\"\", \"ft\"],\n",
    "        [\"gal\", \"gallon\", \"gallons\"],\n",
    "        [\"oz\", \"ounces\", \"ounce\"],\n",
    "        [\"cm\", \"centimeters\", \"centimeter\"],\n",
    "        [\"m\", \"meter\", \"meters\"],\n",
    "        [\"mm\", \"milimeter\", \"millimeter\", \"milimeters\", \"millimeters\"],\n",
    "        [\"a\", \"amp\", \"amps\"],\n",
    "        [\"w\", \"watt\", \"watts\"],\n",
    "        [\"v\", \"volt\", \"volts\"]\n",
    "    ]\n",
    "    \n",
    "    # If we can match the word in an abbreviation group, return the whole group\n",
    "    for g in abrv_groups:\n",
    "        if s in g:\n",
    "            return g\n",
    "\n",
    "    # For values like 3x3, we want to also search 3 by 3\n",
    "    # This is super nasty code but whatever\n",
    "    s_list = list(s)\n",
    "    if len(s_list) > 2:\n",
    "        if s_list[0].isdigit() and s_list[-1].isdigit() and \"x\" in s_list: \n",
    "            for i in xrange(25):\n",
    "                for j in xrange(25):\n",
    "                    if s==\"%sx%s\"%(i,j):\n",
    "                        return [str(i), \"by\", \"xby\", str(j), \"%sby%s\"%(str(i), str(j)), \"%sby%s\"%(str(j), str(i))]\n",
    "        \n",
    "    # If we can't match anything just return an empty array\n",
    "    return []\n",
    "\n",
    "# Turn the string into a series of words\n",
    "def process_string(s):\n",
    "    \n",
    "    # Split into words\n",
    "    words = s.split(\" \")\n",
    "    # Split by dashes if there are any\n",
    "    words = np.hstack(np.array(map(lambda x: x.split(\"-\"), words)))\n",
    "    # Split by x (e.g. 3*3)\n",
    "    words = np.hstack(np.array(map(lambda x: x.split(\"*\"), words)))\n",
    "    # Get rid of commas\n",
    "    words = map(lambda x: x.replace(',', ''), words)\n",
    "    # Get rid of semicolons\n",
    "    words = map(lambda x: x.replace(';', ''), words)\n",
    "    # Get rid of colons\n",
    "    words = map(lambda x: x.replace(':', ''), words)\n",
    "    # Get rid of periods\n",
    "    words = map(lambda x: x.replace('.', ''), words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def pre_process_strings(query, to_match):\n",
    "    \n",
    "    # Lowercase all the things\n",
    "    query = query.lower()\n",
    "    to_match = to_match.lower()\n",
    "    \n",
    "    # Split the query into an array of char arrays\n",
    "    words = process_string(query)\n",
    "    \n",
    "    # Split the matching string\n",
    "    to_match = process_string(to_match)\n",
    "    \n",
    "    # Join words in the query\n",
    "    for i in xrange(1, len(words)):\n",
    "        words.append(words[i] + words[i-1])\n",
    "    \n",
    "    return query, to_match, words\n",
    "\n",
    "\n",
    "# Get a list of words similar to the word if applicable\n",
    "# This will get called with a word in the QUERY\n",
    "def extension_words(word):\n",
    "    \n",
    "    # Make damn sure everything is lower case\n",
    "    w = word.lower()\n",
    "    \n",
    "    # Start building a list of the words we will be returning\n",
    "    # Add abbreviation group if applicable and add the word itself\n",
    "    ret_words = abbrev(w)\n",
    "    ret_words.append(w)\n",
    "    \n",
    "    # If the word is small (<4 chars) or contains a number, only add s and abbreviations\n",
    "    if any(str(i) in w for i in xrange(10)) or len(list(word)) < 4:\n",
    "        ret_words.append(\"%ss\"%w)\n",
    "        return ret_words\n",
    "\n",
    "    # A list of suffixes\n",
    "    suffixes = ['s', 'ed', 'ing', 'n', 'en', 'er', 'est', 'ise', 'fy', 'ly',\n",
    "               'ful', 'able', 'ible', 'hood', 'ess', 'ness', 'less', 'ism',\n",
    "               'ment', 'ist', 'al', 'ish', 'tion']\n",
    "    \n",
    "    # If the word ends in one of these suffixes, add the smaller version\n",
    "    # to strings; otherwise, add this to the end of the word and add that\n",
    "    for x in xrange(len(suffixes)):\n",
    "        l = len(suffixes[x])\n",
    "        if w[-l:] == suffixes[x]:\n",
    "            ret_words.append(w[0:-l])\n",
    "        else:\n",
    "            ret_words.append(w+suffixes[x])\n",
    "\n",
    "    return ret_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for doing word searches\n",
    "These will determine if words in the query are in the matching string. We want to know three basic things:\n",
    "* Does the string contain *any* of the query words?\n",
    "* What fraction of the query words are in the matching words?\n",
    "* What fraction of the chars making up query words are found in the matching words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return the highest fraction of consecutive matching characters\n",
    "# Throw the word out if it's too small or if <1/2 of chars are found\n",
    "#--------------------------------------------------------\n",
    "def match_chars(word, compare):\n",
    "    \n",
    "    strings = extension_words(word)\n",
    "    \n",
    "    # If no words are large enough, return a 0 match rate\n",
    "    if not strings:\n",
    "        return 0\n",
    "    \n",
    "    # Return the sum of characters in the matched words\n",
    "    return sum( map(lambda s: len(word) if s in compare else 0, strings ) )\n",
    "\n",
    "\n",
    "# Determine if the word is in the comparison string\n",
    "#--------------------------------------------------------\n",
    "def match_word(word, compare):\n",
    "    \n",
    "    strings = extension_words(word)\n",
    "    \n",
    "    # If no words are large enough, return a 0 match count\n",
    "    if not strings:\n",
    "        return 0\n",
    "    \n",
    "    # Return the number of words matched\n",
    "    return sum( map(lambda s: 1 if s in compare else 0, strings ) )\n",
    "\n",
    "\n",
    "## STRING MATCHING FUNCTIONS\n",
    "##=========================================================\n",
    "# This will take the search query as well as a string to match it against.\n",
    "# The query will be split into words and those will be split into characters.\n",
    "#   -If any consective number of such characters match the string, it will record that match.\n",
    "#   -The match will be a percentage of the word matching combined with a percentage of the words of the search\n",
    "#      that matched.\n",
    "\n",
    "\n",
    "# Return the number of characters in the matched string divided by the size of the query\n",
    "def char_match_fraction(query, to_match):\n",
    "    \n",
    "    # Process the query into words\n",
    "    query, to_match_words, words = pre_process_strings(query, to_match)\n",
    "    \n",
    "    # Get the number of total matched characters\n",
    "    #matched_chars = sum( filter(lambda x: x != None, map(lambda x: match_chars(x, to_match), words)) )\n",
    "    matched_chars = sum( map(lambda x: match_chars(x, to_match), words) )\n",
    "    \n",
    "    # Return the fraction of matched characters / query size\n",
    "    return float(matched_chars) / len(query)\n",
    "\n",
    "\n",
    "# Get a fraction of words in the query that match the comparison string\n",
    "def word_match_fraction(query, to_match):\n",
    "    \n",
    "    # Process the query into words\n",
    "    query, to_match, words = pre_process_strings(query, to_match)\n",
    "    \n",
    "    # Get the number of matched words\n",
    "    matched_words = sum( map(lambda x: match_word(x, to_match), words) )\n",
    "    \n",
    "    # Return the fraction of words in the query that matched\n",
    "    return float(matched_words) / len(query.split(\" \"))\n",
    "\n",
    "\n",
    "# Return a 1 if the word is matched to a list of strings (words) in the matching string\n",
    "def word_match(query, to_match):\n",
    "    \n",
    "    # Process the query into words\n",
    "    query, to_match_words, query_words = pre_process_strings(query, to_match)\n",
    "    \n",
    "    match = min( 1, sum( map(lambda x: match_word(x, to_match_words), query_words) ) )\n",
    "    return match\n",
    "\n",
    "\n",
    "# Return a 1 if the word is matched in the to_match string\n",
    "def char_match(query, to_match):\n",
    "    \n",
    "    # Process the query into words\n",
    "    query, to_match_words, query_words = pre_process_strings(query, to_match)\n",
    "    \n",
    "    match = min( 1, sum( map(lambda x: match_chars(x, to_match), query_words) ) )\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = \"zip-tie\"\n",
    "char_match_fraction(\"another tieing\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipeline: Part 1\n",
    "I will start by engineering new features and removing the long strings in my data set. Specifically, I want to add\n",
    "\n",
    "* Match rates of query relating to title and description (determined by char_match_fraction function)\n",
    "* String length columns of query, description, and title columns\n",
    "\n",
    "I will go ahead and build a new training set based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "## Utility functions\n",
    "#--------------------------------------------------------\n",
    "\n",
    "# Normalize the column to unit-length 1\n",
    "# Input is a datafram column\n",
    "def norm(col):\n",
    "    # Get the mean\n",
    "    mu = col.mean()\n",
    "    std = col.std()\n",
    "    # Cut off the tails\n",
    "    cp = pd.Series(map(lambda x: (mu-3*std) if x < (mu-3*std) else (mu+3*std) if x > (mu+3*std) else x, col.copy()))\n",
    "    return cp / cp.max()\n",
    "\n",
    "\n",
    "# POOL lambda functions (need to be defined outside the function that calls them)\n",
    "#--------------------------------------------------------\n",
    "# With multiprocessing, we can't use lambda, so I will define some basic functions here\n",
    "def lambda_char_match_fraction(a):\n",
    "    return char_match_fraction( str(a[0]), str(a[1]) )\n",
    "    \n",
    "def lambda_word_match_fraction(a):\n",
    "    return word_match_fraction( str(a[0]), str(a[1]) )\n",
    "\n",
    "def lambda_char_len(a):\n",
    "    l = len( filter(lambda l: l != \" \", list( str(a) )) )\n",
    "    return 0 if np.isnan(l) else l\n",
    "\n",
    "def lambda_word_len(a):\n",
    "    l = len(str(a).split(\" \"))\n",
    "    return l if l > 1 else 0\n",
    "\n",
    "def lambda_in_attr(a):\n",
    "    return ATTR_ARR[str(int(a))] if str(int(a)) in ATTR_ARR else ''\n",
    "\n",
    "def lambda_word_match(a):\n",
    "    return word_match( str(a[0]), str(a[1]) )\n",
    "\n",
    "def lambda_char_match(a):\n",
    "    return char_match( str(a[0]), str(a[1]) )\n",
    "\n",
    "## DATA PIPELINE\n",
    "#--------------------------------------------------------\n",
    "# Given the data (train or test) and description files,\n",
    "# perform a series of operations to produce a data set on which we can do ML\n",
    "def pipeline(data_file, **kwargs):\n",
    "    \n",
    "    # Define my multiprocessing pool and start the timer\n",
    "    POOL = Pool(maxtasksperchild=1000)\n",
    "    start = time.time()\n",
    "    \n",
    "    ## Read files\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # Read the initial train.csv and join it to product descriptions\n",
    "    _df = pd.read_csv(data_file)\n",
    "\n",
    "    # Add in descriptions because they are 1:1\n",
    "    df = pd.merge(_df, pd.read_csv(f_desc), how='outer')\n",
    "    \n",
    "\n",
    "    \n",
    "    ## ADD MATCH COLUMNS\n",
    "    #--------------------------------------------------------\n",
    "\n",
    "    # Description columns\n",
    "    desc_zip = np.dstack( ( np.array(df['search_term']), np.array(df['product_description']) ))[0]\n",
    "    df['desc_char'] = norm(pd.Series( POOL.map(lambda_char_match_fraction, desc_zip ) ))\n",
    "    df['desc_word'] = norm(pd.Series( POOL.map(lambda_word_match_fraction, desc_zip ) ))\n",
    "    df['desc_word_bin'] = pd.Series( POOL.map(lambda_word_match, desc_zip) )\n",
    "    df['desc_char_bin'] = pd.Series( POOL.map(lambda_char_match, desc_zip) )\n",
    "    \n",
    "    \n",
    "    # Title columns\n",
    "    title_zip = np.dstack( (np.array(df['search_term']), np.array(df['product_title']) ))[0]\n",
    "    df['title_char'] = norm( pd.Series( POOL.map(lambda_char_match_fraction, title_zip ) ))\n",
    "    df['title_word'] = norm( pd.Series( POOL.map(lambda_word_match_fraction, title_zip ) ))\n",
    "    df['title_word_bin'] = pd.Series( POOL.map(lambda_word_match, title_zip) )\n",
    "    df['title_char_bin'] = pd.Series( POOL.map(lambda_char_match, title_zip) )\n",
    "\n",
    "    \n",
    "    # Combo columns\n",
    "    #df['desc_char_word'] = df['desc_char'] * df['desc_word']\n",
    "    #df['title_char_word'] = df['title_char'] * df['title_word']\n",
    "    #df['desc_title_char'] = df['desc_char'] * df['title_char']\n",
    "    #df['desc_title_word'] = df['desc_word'] * df['title_word']\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ADD LENGTH Columns (filter out whitespace)\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # Char lengths\n",
    "    df['desc_char_l'] = norm( pd.Series( POOL.map(lambda_char_len, df['product_description']) ))\n",
    "    df['title_char_l'] = norm( pd.Series( POOL.map(lambda_char_len, df['product_title']) ))\n",
    "    df['query_char_l'] =  norm( pd.Series( POOL.map(lambda_char_len, df['search_term']) ))\n",
    "\n",
    "    # Word lengths\n",
    "    df['desc_word_l'] = norm( pd.Series( POOL.map(lambda_word_len, df['product_description']) ))\n",
    "    df['title_word_l'] = norm( pd.Series( POOL.map(lambda_word_len, df['product_title']) ))\n",
    "    df['query_word_l'] = norm( pd.Series( POOL.map(lambda_word_len, df['search_term']) ))\n",
    "\n",
    "    \n",
    "    \n",
    "    ## ADD ATTRIBUTE columns\n",
    "    #---------------------------------------------------------\n",
    "    # First we need the attr column added to the df\n",
    "    df['attr'] = POOL.map(lambda_in_attr, df['product_uid'])\n",
    "    \n",
    "    # Now build the columns normally\n",
    "    attr_zip = np.dstack( (np.array(df['search_term']), np.array(df['attr']) ))[0]\n",
    "    df['attr_char'] = norm(pd.Series( POOL.map(lambda_char_match_fraction, attr_zip ) ))\n",
    "    df['attr_word'] = norm(pd.Series( POOL.map(lambda_word_match_fraction, attr_zip ) ))\n",
    "    df['attr_char_l'] = norm( pd.Series( POOL.map(lambda_char_len, df['attr']) ))\n",
    "    df['attr_word_l'] = norm( pd.Series( POOL.map(lambda_word_len, df['attr']) ))\n",
    "    df['attr_word_bin'] = pd.Series( POOL.map(lambda_word_match, attr_zip) )\n",
    "    df['attr_char_bin'] = pd.Series( POOL.map(lambda_char_match, attr_zip) )\n",
    "    \n",
    "    ## REMOVE TEXT columns\n",
    "    #--------------------------------------------------------\n",
    "    map(lambda x: df.pop(x), ['product_uid', 'search_term', 'product_title', 'product_description', 'attr'])\n",
    "\n",
    "    print \"df size: %s\"%str(np.shape(df))\n",
    "    ## DROP ROWS NaN values (but only if kwargs does not include submission)\n",
    "    if 'submission' in kwargs:\n",
    "        clean_df = df.copy()\n",
    "    else:\n",
    "        clean_df = df.copy().dropna()\n",
    "    \n",
    "    ## Pop off the ids\n",
    "    ids = clean_df['id']\n",
    "    clean_df.pop('id')\n",
    "    \n",
    "    print \"clean_df size: %s\"%str(np.shape(clean_df))\n",
    "    print display(HTML(\"<font color='blue'><b>Data pipelined in %s s</b></font>\"%(time.time()-start)))\n",
    "    return clean_df, ids\n",
    "\n",
    "## For submission, we need ids in order\n",
    "## This function pops off the id column and returns it as a series\n",
    "def get_id_col(data_file):\n",
    "    df = pd.read_csv(data_file)\n",
    "    print \"df shape: %s\"%str(np.shape(df))\n",
    "    new_df = df.copy().dropna()\n",
    "    print \"new_df shape: %s\"%str(np.shape(new_df))\n",
    "    return pd.Series(new_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df, ids = pipeline(f_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Feature Distributions\n",
    "As a sanity check, it is good to check out the first few lines of my data frame and also to graph the features to make sure there are actual distributions of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution (histogram) of my features\n",
    "def plot_hist(col, name):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    plt.title('%s' %name, fontsize=15)\n",
    "    #fig.colorbar(cax)\n",
    "    plt.hist(col)\n",
    "    #plt.xlabel('Value')\n",
    "    #plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature plots\n",
    "features = list(df.columns.values)\n",
    "\n",
    "# Plot a bunch of stuff\n",
    "dim = len(features)/3 + 1 if len(features)%3 > 0 else len(features)/3\n",
    "\n",
    "f, axarr = plt.subplots(dim, 3, figsize=(16,20))\n",
    "plt.tight_layout(pad=3)\n",
    "\n",
    "for i in range(0, dim):\n",
    "    # For each row\n",
    "    for j in range(0, 3):\n",
    "        # For each element in the row\n",
    "        if (i*3 + j) < len(features):\n",
    "            # As long as the chart exists in the tuple\n",
    "            axarr[i][j].hist( df[ features[i*3+j] ], color='orange' )\n",
    "            axarr[i][j].set_title( features[i*3+j], fontsize=15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set: Distribution of Relevance Scores\n",
    "I also want to take a look at how the relevance scores are distributed in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(df['relevance'], 'Normalized Relevance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "A few notes about the distributions:\n",
    "\n",
    "* The string length columns look to be distributed pretty nicely\n",
    "* The description matches are heavily favored to the right (meaning the strings match well); we would expect this from a search engine\n",
    "* The relevance scores are also heavily favored to the right (again, we expect this engine to work reasonably well, so this makes makes sense)\n",
    "\n",
    "Everything so far looks reasonable. Now I will go ahead and set up a machine learning pipeline to test some algorithms on the training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since relevance scores are R [1,3], we can divide them by 3 to put them in the \"norm\" range\n",
    "def divide_y(y):\n",
    "    return y/3.\n",
    "\n",
    "# Move relevance over to y\n",
    "if 'relevance' in df:\n",
    "    train_y = divide_y(pd.Series(df['relevance']))\n",
    "    df.pop('relevance')\n",
    "\n",
    "    # Rename df\n",
    "    train_X = np.array(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "## CROSS VALIDATION\n",
    "##================================================================\n",
    "## Cross-validate and return score\n",
    "def cv_score(X, y, folds, model):\n",
    "    # Get an array of scores\n",
    "    scores = cross_validation.cross_val_score(model, X, y, cv=folds, n_jobs=-1)\n",
    "    # Return mean and std\n",
    "    return (abs(np.mean(scores)), np.std(scores))\n",
    "\n",
    "## Cross-validate and return the a predicted y vector\n",
    "def cv_fit(X, y, folds, model):\n",
    "    return cross_validation.cross_val_fit(model, X, y, cv=folds, n_jobs=-1)\n",
    "\n",
    "\n",
    "## MODEL OPTIMIZATION\n",
    "##=================================================================\n",
    "# Optimize the number of CV folds\n",
    "def tune_folds(X, y, MODEL, **kwargs):\n",
    "    \n",
    "    # Range of folds\n",
    "    min_i = kwargs['min_i'] if 'min_i' in kwargs else 3\n",
    "    max_i = kwargs['max_i'] if 'max_i' in kwargs else 10\n",
    "    f = [i for i in xrange(min_i, max_i+1)] \n",
    "    \n",
    "    # Get the scores\n",
    "    scores = map(lambda i: {'folds': i, 'score': cv_score(X, y, i, MODEL)}, f)\n",
    "    \n",
    "    # Plot means\n",
    "    plt.plot(f, map(lambda x: x['score'][0], scores))\n",
    "    return scores   \n",
    "    \n",
    "    \n",
    "# Map an array of param values to an array of CV scores and plot it\n",
    "#    @ models is an L-dimensional list of models instantiated with the param value\n",
    "#    @ labels is an L-dimensional list of labels corresponding 1:1 with models being tested\n",
    "#--------------------------------------------------------\n",
    "def tune_model(X, y, model, **kwargs):\n",
    "    \n",
    "    start = time.time()\n",
    "    folds = 3\n",
    "    \n",
    "    args = kwargs['args']\n",
    "    static = kwargs['static']\n",
    "    \n",
    "    # Iterate through the args\n",
    "    for arg, val in args.iteritems():\n",
    "        \n",
    "        # Copy the static args to a new set of args and add the arg we're optimizing\n",
    "        def append_arg(static, arg, val):\n",
    "            static[arg] = val\n",
    "            return static\n",
    "        \n",
    "        # Init the models with pointers to updated static arguments\n",
    "        # Note that static arguments can be updated 1 of 2 ways:\n",
    "        #    1: Before calling this function (tune_model)\n",
    "        #    2: By appending a dynamic arg (which we are trying to optimize) using append_arg\n",
    "        _models = map(lambda x: model( **append_arg(static, arg, x) ), args[arg])\n",
    "        \n",
    "        # Get the scores (CV is itself multi-processed so I won't use a pool here)\n",
    "        scores = map(lambda m: cv_score(X, y, folds, m), _models)\n",
    "        \n",
    "        # Plot means; plot categorical variables in a bar chart and quantitative ones in a line chart\n",
    "        plt.figure()\n",
    "        \n",
    "        if isinstance( args[arg][0], str):\n",
    "            left = [i for i in xrange(len(args[arg]))]\n",
    "            plt.bar(left, map(lambda x: x[0], scores), width=0.5, tick_label=args[arg], align='center')\n",
    "        else:\n",
    "            plt.plot(args[arg], map(lambda x: x[0], scores))\n",
    "        \n",
    "        plt.title(arg, fontsize=16)\n",
    "    \n",
    "    \n",
    "    #display(HTML(\"<font color='blue'>Best MSE: %s</font>\"%(global_mse) ))\n",
    "    #display(HTML(\"<font color='blue'>Trained model in %s s</font>\"%(time.time()-start)))\n",
    "\n",
    "    return\n",
    "\n",
    "# Once all of the dynamic args have been turned into static args, evaluate the model\n",
    "# Note: ALL models are trained on 3 CV folds\n",
    "def eval_model(X, y, model):\n",
    "    (mean, std) = cv_score(X, y, 3, model)\n",
    "    display(HTML(\"<b>Model optimized with MSE: %s +/- %s</b>\"%(mean, std)))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Methods\n",
    "Here I will look at some vanilla regression methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"args = {\n",
    "    'alpha': [i/10. for i in range(0, 10)],\n",
    "}\n",
    "r_args = {'args': args, 'static': {}}\n",
    "\n",
    "tune_model(train_X, train_y, Ridge, **r_args )\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "Here I will start by exploring a few ensemble methods and see where they take me. Reminder that this is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor as ABR\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.ensemble import RandomForestRegressor as RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"abr_dynamic_args = {\n",
    "    #'loss': ['linear', 'exponential', 'square'],\n",
    "    #'n_estimators': [10, 20, 30, 40, 50, 60],\n",
    "    #'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.]\n",
    "}\n",
    "\n",
    "abr_static_args = {\n",
    "    'loss': 'linear',\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.8\n",
    "}\n",
    "\n",
    "abr_args = {'args': abr_dynamic_args, 'static': abr_static_args}\n",
    "tune_model(train_X, train_y, ABR, **abr_args)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#eval_model(train_X, train_y, ABR(**abr_static_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr_dynamic_args = {\n",
    "    #'alpha': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    #'n_estimators': [2,4,6,8,10],\n",
    "    #'min_samples_split': [4,5,6,7,8,9],\n",
    "    #'min_samples_leaf': [1, 2, 3, 4],\n",
    "    #'min_weight_fraction_leaf': [0.0, 0.05 ,0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    #'max_depth': [2,5,10],\n",
    "    #'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.09],\n",
    "    #'loss': ['ls', 'lad', 'huber', 'quantile']\n",
    "}\n",
    "\n",
    "# After working through the above, I optimized a few of the params\n",
    "gbr_static_args = {\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 0.9,\n",
    "    'min_samples_leaf': 3,\n",
    "    #'min_samples_split': 5,\n",
    "    'loss': 'ls',\n",
    "    #'min_weight_fraction_leaf': 0.05,\n",
    "    'max_depth': 4\n",
    "}\n",
    "\n",
    "#gbr_args = {'args': gbr_dynamic_args, 'static': gbr_static_args}\n",
    "#tune_model(train_X, train_y, GBR, **gbr_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#eval_model(train_X, train_y, GBR(**gbr_static_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_dynamic_args = {\n",
    "    #'n_estimators': [5, 10, 15, 20, 25, 30],\n",
    "    #'max_depth': [None, 2, 3, 4],\n",
    "    #'min_samples_split': [1, 2, 3, 4, 5],\n",
    "    #'min_samples_leaf': [1, 2, 3, 4],\n",
    "    #'min_weight_fraction_leaf': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "}\n",
    "\n",
    "rf_static_args = {\n",
    "    'n_estimators': 350,\n",
    "    'min_samples_split': 2,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "rf_args = {'args': rf_dynamic_args, 'static': rf_static_args}\n",
    "tune_model(train_X, train_y, RF, **rf_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the Model\n",
    "Now that I have tested various models with CV, I will fit the best one to the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#M = ABR(**abr_static_args)\n",
    "#M = GBR(**gbr_static_args)\n",
    "M = RF(**rf_static_args)\n",
    "M.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set\n",
    "Now I will move over to the test set. I will predict based on the model I just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test, df_test_ids = pipeline(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a new X and predict y using the model we fit earlier\n",
    "test_X = np.array(df_test.copy())\n",
    "test_y = M.predict(test_X)\n",
    "\n",
    "final_y = map(lambda x: 1 if x<(1./3) else 3 if x>(1.) else x*3., test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the data, I want to look at the distribution and compare it to the one from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(final_y, 'Scores in Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_train_y = map(lambda x: 1 if x<(1./3) else 3 if x>(1.) else x*3., train_y)\n",
    "plot_hist(new_train_y, 'Normalized Relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Test mean: %s, std: %s\"%(np.mean(final_y), np.std(final_y))\n",
    "print \"Training mean: %s, std: %s\"%(np.mean(new_train_y), np.std(new_train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Now I am finally ready to write the submission file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Join ids with submission y\n",
    "def submit(ids, relevances, file_name):\n",
    "    \n",
    "    # ids need to be integers\n",
    "    ids = map(lambda x: int(x), ids)\n",
    "    \n",
    "    # Build a dataframe\n",
    "    submission = pd.DataFrame(index=ids)\n",
    "    submission.index.name = 'id'\n",
    "    submission['relevance'] = relevances\n",
    "    \n",
    "    # Print the head just for a sanity check\n",
    "    submission.head(10)\n",
    "    \n",
    "    # Write the file\n",
    "    path = \"%s/%s.csv\"%(DATADIR, file_name)\n",
    "    submission.to_csv(path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit(df_test_ids, final_y, 'submission2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"%ssubmission2.csv\"%DATADIR)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
