{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "### TVID transformers for product_title and product_description throw errors because the encoding for those columns is whack (and when i read_csv and encode in encoding=\"ISO-8859-1\" it doesn't pipeline the features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Product Search Relevance\n",
    "The goal of this analysis is to determine how to predict relevance of a search on Home Depot's website. The training data were labelled by crowdsourcing humans, but the hope is that the text and numerical features will be enough to predict relevance via machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import my library stack\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import gc\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "# Some nice display tools for ipython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# There are several files that the Kaggle competition included for this analysis\n",
    "DATADIR = \"%s/home_depot_2015/\"%os.environ[\"KAGGLE_DATA_DIR\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Overview of the data\n",
    "There are three files I will take a peek at here:\n",
    "    \n",
    "* **train.csv** -- The training set, which contains products, searches, and relevance scores\n",
    "* **test.csv** -- The test set, which contains products and searches --> I am to predict relevance scores\n",
    "    \n",
    "* **product_descriptions.csv** -- Contains product id and a plain text description of the product\n",
    "    \n",
    "* **attributes.csv** -- Contains product id and several attributes, but for only a *subset* of products\n",
    "\n",
    "I'm just going to preview the first few rows of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>First three rows of train</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                      product_title    search_term  \\\n",
       "0   2       100001  Simpson Strong-Tie 12-Gauge Angle  angle bracket   \n",
       "\n",
       "   relevance  \n",
       "0          3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>First three rows of test</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>90 degree bracket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                      product_title        search_term\n",
       "0   1       100001  Simpson Strong-Tie 12-Gauge Angle  90 degree bracket"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>First three rows of descriptions</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                product_description\n",
       "0       100001  Not only do angles make joints stronger, they ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>First three rows of attributes</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet01</td>\n",
       "      <td>Versatile connector for various 90Â° connectio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid      name                                              value\n",
       "0       100001  Bullet01  Versatile connector for various 90Â° connectio..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first 5 rows of a csv file given the path to it\n",
    "def preview_data(file, name):\n",
    "    print display(HTML(\"<h3>First three rows of %s</h3>\"%name))\n",
    "    preview_df = pd.read_csv(file, encoding=\"ISO-8859-1\")\n",
    "    print display(preview_df.head(1))\n",
    "\n",
    "def get_path(file):\n",
    "    return \"%s%s\"%(DATADIR, file)\n",
    "\n",
    "# Define the files for later\n",
    "f_train = get_path('train.csv')\n",
    "f_test = get_path('test.csv')\n",
    "f_desc = get_path('product_descriptions.csv')\n",
    "f_attr = get_path('attributes.csv')\n",
    "\n",
    "# Do all four\n",
    "files = [(f_train, 'train'), (f_test, 'test'), (f_desc, 'descriptions'), (f_attr, 'attributes')]\n",
    "map(lambda x: preview_data(x[0], x[1]), files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Feature engineering\n",
    "After looking at these spreadsheets, I realize there isn't a ton of information with which to work. My initial thought is to do some sort of a word matching procedure (e.g. see if one of the search words matches one of the words in the title (or description, or attributes). Better still, I could take the individual letters in each word of the search query and try to see if they appear consecutively in the raw string of the title, description, or attributes.\n",
    "\n",
    "Let's give that a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I will definitely want to use multiprocessing in the coming steps\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom manipulation of attributes\n",
    "The attributes file has a dump of attributes and their respective product_uid values. I want two things out of this file:\n",
    "* A concatenation of all the \"values\"; that is, all of the raw text information\n",
    "* Specifically the brand name (marked as \"MFG Brand Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The attributes are a little trickier. There may be 0 or many attributes per 1 product_uid\n",
    "# I want to concatenate all strings belonging to a particular product_uid\n",
    "\n",
    "# Data[0] = product_uid; data[1] = name; data[2] = value\n",
    "def collapse_attr(data):\n",
    "    attr = {}\n",
    "    for d in data:\n",
    "        # d is an array of form [product_uid, name, value]\n",
    "        if not np.isnan(d[0]):\n",
    "            i = str(int(d[0]))\n",
    "            # Concatenate the attribute as a string\n",
    "            # Also add the brand if it exists\n",
    "            if i in attr:\n",
    "                attr[i]['string'] = \"%s %s\"%( attr[i]['string'], str(d[2]) )\n",
    "                if d[1] == 'MFG Brand Name':\n",
    "                    attr[i]['brand'] = str(d[2])\n",
    "            else:\n",
    "                attr[i] = {'string': str(d[2])}\n",
    "                \n",
    "                if d[1] == 'MFG Brand Name':\n",
    "                    attr[i]['brand'] = str(d[2])\n",
    "                else:\n",
    "                    attr[i]['brand'] = ''\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training attribute array, which we will append to the dataframe in the pipeline\n",
    "attr = pd.read_csv(f_attr)\n",
    "ATTR_ARR = collapse_attr(np.array(attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for processing the strings\n",
    "These will format the strings, add alternate suffixes, and add some common abbreviations if applicable. Since we're dealing with Home Depot data, we have a general idea of what types of abbreviations we might encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a word, return all forms of it and its abbreviations\n",
    "def abbrev(s):\n",
    "    abrv_groups = [\n",
    "        [\"'\", \"in\", \"inches\", \"inch\"],\n",
    "        [\"pounds\", \"pound\", \"lbs\", \"lb\"],\n",
    "        [\"sqft\", \"sq\", \"sf\", \"square\", \"squared\", \"foot\", \"feet\", \"\\\"\", \"ft\", \"inch\", \"inches\", \"in\", \"'\"],\n",
    "        [\"cf\", \"cu\", \"cubic\", \"cubed\", \"inch\", \"foot\", \"feet\", \"'\", \"\\\"\", \"ft\", \"in\", \"inches\"],\n",
    "        [\"gal\", \"gallon\", \"gallons\"],\n",
    "        [\"g\", \"gram\", \"grams\", \"kg\", \"kilogram\", \"kilo\"],\n",
    "        [\"oz\", \"ounces\", \"ounce\"],\n",
    "        [\"cm\", \"centimeters\", \"centimeter\"],\n",
    "        [\"m\", \"meter\", \"meters\"],\n",
    "        [\"mm\", \"milimeter\", \"millimeter\", \"milimeters\", \"millimeters\"],\n",
    "        [\"a\", \"amp\", \"amps\", \"ampere\", \"amperes\"],\n",
    "        [\"w\", \"watt\", \"watts\"],\n",
    "        [\"v\", \"volt\", \"volts\"],\n",
    "        [\"whirpool\",\"whirlpool\", \"whirlpoolga\", \"whirlpoolstainless\",\"stainless\"],\n",
    "        [\"and\", \"&\", \"+\", \"&amp;\"],\n",
    "        [\"x\", \"by\", \"*\"],\n",
    "        [\"deg\", \"degree\", \"degrees\", \"°\", \"angle\"],\n",
    "        ['dia', 'diameter']\n",
    "    ]\n",
    "    \n",
    "    # If we can match the word in an abbreviation group, return the whole group\n",
    "    for g in abrv_groups:\n",
    "        if s in g:\n",
    "            return g\n",
    "        \n",
    "    # If we can't match anything just return an empty array\n",
    "    return []\n",
    "\n",
    "# Turn the string into a series of words\n",
    "def process_string(s):\n",
    "    \n",
    "    __words = np.array(s.split(\" \"))\n",
    "    \n",
    "    # Split by special, but include those characters\n",
    "    # This regex splits by the characters [', \", /, *, -], but INCLUDES those characters\n",
    "    _words = list(np.hstack(map(lambda i: re.split(r\"(plus|[('\\\"\\-*/)])\", i), __words)))\n",
    "\n",
    "    words = _words\n",
    "    \n",
    "    # Get rid of commas\n",
    "    words = map(lambda x: x.replace(',', ''), words)\n",
    "    # Get rid of semicolons\n",
    "    words = map(lambda x: x.replace(';', ''), words)\n",
    "    # Get rid of colons\n",
    "    words = map(lambda x: x.replace(':', ''), words)\n",
    "    # Get rid of periods\n",
    "    words = map(lambda x: x.replace('.', ''), words)\n",
    "    # Get rid of blanks\n",
    "    words = filter(lambda x: x != ' ' and x != '', words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def pre_process_strings(query):\n",
    "    \n",
    "    # Lowercase all the things\n",
    "    query = str(query).lower()\n",
    "    \n",
    "    # Split the query into an array of char arrays\n",
    "    query_words = process_string(query)\n",
    "\n",
    "    return query_words\n",
    "\n",
    "\n",
    "# This function processes strings like \"4x4\" or \"4'x4'\"\n",
    "def process_x_by(s):\n",
    "    if any(i.isdigit() for i in s) and \"x\" in s:\n",
    "        new_s = list(filter(lambda x: x!='', s.split(\"x\"))); new_s.append(s); new_s.append(\"x\")\n",
    "        return new_s\n",
    "    else:\n",
    "        return [s]\n",
    "\n",
    "# Split strings that have unit suffixes (e.g. 10in, 50g, etc)\n",
    "def process_unit_suffixes(s):\n",
    "    strings = [\"mm\", \"cm\", \"m\", \"g\", \"kg\", \"in\", \"ft\", \"a\", \"w\", \"v\", \"oz\", \"gal\", \"cf\"]\n",
    "    # Inefficient but I can't think of a better way\n",
    "    for i in strings:\n",
    "        # If any of the above strings is in s, return that string + the number\n",
    "        if i in s and any(j.isdigit() for j in s):\n",
    "            arr = list(filter(lambda x: x!='', s.split(i))); arr.append(i); arr.append(s)\n",
    "            return arr\n",
    "\n",
    "    return [s]\n",
    "\n",
    "# Get a list of words similar to the word if applicable\n",
    "# This will get called with a word in the QUERY\n",
    "def extension_words(word):\n",
    "    \n",
    "    # Make damn sure everything is lower case\n",
    "    w = word.lower()\n",
    "    \n",
    "    # Process strings of the form \"AxB\"\n",
    "    ret_words = process_x_by(w)\n",
    "    \n",
    "    # Include abbreviation words\n",
    "    abbr = abbrev(w)\n",
    "    \n",
    "    # Flatten array\n",
    "    ret_words = list(np.hstack([ret_words, abbr]))\n",
    "    \n",
    "    \n",
    "    # If the word is small (<4 chars), contains a number, or contains a special character,\n",
    "    #     only add s and return\n",
    "    if any(i.isdigit() for i in w) or len(list(word)) < 4 or any(i in w for i in [\"-\", \"*\", \"'\", \"\\\"\", \"/\"]):\n",
    "        ret_words.append(\"%ss\"%w)\n",
    "        return filter(lambda x: x!='' and x!=' ', ret_words)\n",
    "\n",
    "    \n",
    "    # A list of suffixes\n",
    "    suffixes = ['s', 'ed', 'ing', 'n', 'en', 'er', 'est', 'ise', 'fy', 'ly',\n",
    "               'ful', 'able', 'ible', 'hood', 'ess', 'ness', 'less', 'ism',\n",
    "               'ment', 'ist', 'al', 'ish', 'tion']\n",
    "    \n",
    "    # If the word ends in one of these suffixes, add the smaller version\n",
    "    # to strings; otherwise, add this to the end of the word and add that\n",
    "    for x in xrange(len(suffixes)):\n",
    "        l = len(suffixes[x])\n",
    "        if w[-l:] == suffixes[x]:\n",
    "            ret_words.append(w[0:-l])\n",
    "        else:\n",
    "            ret_words.append(w+suffixes[x])\n",
    "\n",
    "    return filter(lambda x: x!='' and x!=' ', ret_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for doing word searches\n",
    "These will determine if words in the query are in the matching string. We want to know three basic things:\n",
    "* Does the string contain *any* of the query words?\n",
    "* What fraction of the query words are in the matching words?\n",
    "* What fraction of the chars making up query words are found in the matching words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine if the word is in the comparison string\n",
    "#   @returns 1 or 0\n",
    "def match_word(word, compare):\n",
    "    strings = extension_words(word)\n",
    "    if not strings: return 0\n",
    "    return min(1, sum( map(lambda s: 1 if s in compare else 0, strings ) ))\n",
    "\n",
    "# Determine the number of times the word (or any version of it) matches a string\n",
    "#   @returns array of match counts\n",
    "def match_word_count(word, compare):  \n",
    "    strings = filter( lambda x: x!='' and x!=' ', extension_words(word) )\n",
    "    if not strings: return 0\n",
    "    return max( map(lambda s: compare.count(s) , strings ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "## STRING MATCHING\n",
    "##=========================================================\n",
    "\n",
    "# Get the number of unique words that are matched\n",
    "def matched_words_string(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    return sum( map(lambda x: match_word(x, to_match.lower()), query_words) ) if query_words else 0\n",
    "    \n",
    "    \n",
    "# Get the count of all words matched (i.e. if a word is matched more than once, it is counted multiple times)\n",
    "def count_matched_words_string(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    return sum( map(lambda x: match_word_count(x, to_match.lower()), query_words) )\n",
    "\n",
    "## Word matching to the words in the string\n",
    "def matched_words_words(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    to_match_words = to_match.split(\" \")\n",
    "    sums = map(lambda y: sum( map(lambda x: match_word(x, to_match.lower()), query_words) ), \\\n",
    "                to_match_words) if query_words else 0\n",
    "    return sum(sums)\n",
    "\n",
    "# Count of words matching the words in the string\n",
    "def count_matched_words_words(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    to_match_words = to_match.split(\" \")\n",
    "    counts = map(lambda y: sum( map(lambda x: match_word_count(x, to_match.lower()), query_words) ), \\\n",
    "                to_match_words) if query_words else 0\n",
    "    return sum(counts)\n",
    "        \n",
    "\n",
    "\n",
    "## QUERY MATCHING\n",
    "##=========================================================\n",
    "\n",
    "# Whether or not the whole query is in the string\n",
    "def matched_query(query, to_match):\n",
    "    return query in to_match\n",
    "\n",
    "# How many times the whole query is in the string\n",
    "def count_matched_query(query, to_match):\n",
    "    return to_match.count(query)\n",
    "\n",
    "# Whether or not the last word of the query is in the to_match string\n",
    "def last_word(query, to_match):\n",
    "    last_q = query.split(\" \")[-1]\n",
    "    return 1 if last_q in to_match else 0\n",
    "\n",
    "# Whether or not a word from the query is the FIRST word in the to_match string\n",
    "def first_word(query, to_match):\n",
    "    first_q = query.split(\" \")[0]\n",
    "    return 1 if first_q in to_match else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Feature Engineering Pipeline\n",
    "I will start by engineering new features and removing the long strings in my data set. Specifically, I want to add\n",
    "\n",
    "* Match rates of query relating to title and description (determined by char_match_fraction function)\n",
    "* String length columns of query, description, and title columns\n",
    "\n",
    "I will go ahead and build a new training set based on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions and Pool\n",
    "This is a lot of extra code but multiprocessing will speed things up significantly and the lambda functions have to be defined in scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# POOL lambda functions (need to be defined outside the function that calls them)\n",
    "# With multiprocessing, we can't use lambda, so I will define some basic functions here\n",
    "#================================================================================\n",
    "import string\n",
    "import sys\n",
    "printable = set(string.printable)\n",
    "\n",
    "# An encoding function because apparently some of these aren't readable as utf-8...\n",
    "def encode(x):\n",
    "    return filter(lambda i: i in printable, x)\n",
    "    #return filter(lambda i: i in printable, x.encode('utf-8').strip())\n",
    "\n",
    "def en(strs):\n",
    "    if isinstance(strs, str):\n",
    "        return encode(strs)\n",
    "    else:\n",
    "        return strs\n",
    "    \"\"\"\n",
    "    # If it's a single item, encode it right away\n",
    "    if isinstance(strs, (float, int, np.float64, np.int64)):\n",
    "        return encode(str(int(strs)))\n",
    "    elif isinstance(strs, str):\n",
    "        return encode(strs)\n",
    "    \n",
    "    # If it's an array, encode an array\n",
    "    elif isinstance(strs, list):\n",
    "        ret_strs = []\n",
    "        for x in strs:\n",
    "            # We want to encode this item in utf-8 and remove all non-ascii characters\n",
    "            if isinstance(x, (float, int, np.float64, np.int64, str)):\n",
    "                return encode(str(x))\n",
    "            else:\n",
    "                ret_strs.append(x)\n",
    "        return ret_strs\n",
    "    else:\n",
    "        return strs\n",
    "    \"\"\"\n",
    "    \n",
    "# Attribute stuff\n",
    "def lambda_in_attr(a):\n",
    "    a = str(int(float(en(a))))\n",
    "    return ATTR_ARR[a]['string'] if a in ATTR_ARR else ''\n",
    "def lambda_brand(a):\n",
    "    a = str(int(float(en(a))))\n",
    "    return ATTR_ARR[a]['brand'] if a in ATTR_ARR else ''\n",
    "\n",
    "\n",
    "# Lengths\n",
    "def lambda_char_len(a):\n",
    "    return float(len(list(str(a))))\n",
    "def lambda_word_len(a):\n",
    "    return float(len(str(a).split(\" \")))\n",
    "\n",
    "\n",
    "# Matched words\n",
    "def l_matched_words_string(a):\n",
    "    try:\n",
    "        return float(matched_words_string( str(a[0]), str(a[1]) ))\n",
    "    except:\n",
    "        print a\n",
    "        return\n",
    "    \n",
    "def l_count_matched_words_string(a):\n",
    "    return float(count_matched_words_string( str(a[0]), str(a[1]) ))\n",
    "def l_matched_words_words(a):\n",
    "    return float(matched_words_words( str(a[0]), str(a[1]) ))\n",
    "def l_count_matched_words_words(a):\n",
    "    return float(count_matched_words_words( str(a[0]), str(a[1]) ))\n",
    "\n",
    "\n",
    "# Matched queries\n",
    "def l_matched_query(a):\n",
    "    return matched_query( str(a[0]), str(a[1]) )\n",
    "def l_count_matched_query(a):\n",
    "    return float(count_matched_query( str(a[0]), str(a[1]) ))\n",
    "\n",
    "\n",
    "# Binaries\n",
    "def l_last_word(a):\n",
    "    return last_word( str(a[0]), str(a[1]) )\n",
    "def l_first_word(a):\n",
    "    return first_word( str(a[0]), str(a[1]) )\n",
    "\n",
    "\n",
    "# An abstracted multiprocessor functional so that I can remove MP when debugging\n",
    "# All functions are applied to a map\n",
    "POOL = Pool(maxtasksperchild=1000)\n",
    "def run_pool(f, iterator):\n",
    "    #m = map(f, iterator)\n",
    "    m = POOL.map(f, iterator)\n",
    "    return pd.Series(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature pipeline\n",
    "Here I will add all of my features to the dataframe that will be used as X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "#================================================================================\n",
    "## DATA PIPELINE\n",
    "#================================================================================\n",
    "# Given the data (train or test) and description files,\n",
    "# perform a series of operations to produce a data set on which we can do ML\n",
    "def feature_pipeline(data_file, **kwargs):\n",
    "    \n",
    "    # Define my multiprocessing pool and start the timer\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    #============\n",
    "    # Read files\n",
    "    #============\n",
    "    \n",
    "    # Read the initial train.csv and join it to product descriptions\n",
    "    #_df = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
    "    _df = pd.read_csv(data_file)\n",
    "\n",
    "    # Add in descriptions because they are 1:1\n",
    "    df = pd.merge(_df, pd.read_csv(f_desc), how='outer')\n",
    "    \n",
    "    # If there is an attribute for a product uid, join it\n",
    "    df['attr'] = run_pool(lambda_in_attr, df['product_uid'])\n",
    "    df['brand'] = run_pool(lambda_brand, df['product_uid'])\n",
    "    \n",
    "    \n",
    "    # Construct the columns to be added to the dataframe. Each will be added with run_pool defined above.\n",
    "    \n",
    "    # Char lengths\n",
    "    char_lengths = [\n",
    "        ['desc_char_l', lambda_char_len, df['product_description'] ],\n",
    "        ['title_char_l', lambda_char_len, df['product_title'] ],\n",
    "        ['query_char_l', lambda_char_len, df['search_term'] ],\n",
    "        ['attr_char_l', lambda_char_len, df['attr'] ],\n",
    "        ['brand_char_l', lambda_char_len, df['brand'] ]\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Word lengths\n",
    "    word_lengths = [\n",
    "        ['desc_word_l', lambda_word_len, 'product_description'],\n",
    "        ['title_word_l', lambda_word_len, 'product_title'],\n",
    "        ['query_word_l', lambda_word_len, 'search_term'],\n",
    "        ['attr_word_l', lambda_word_len, 'attr'],\n",
    "        ['brand_word_l', lambda_word_len, 'brand']\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Zip the data into tuples\n",
    "    desc_zip = np.dstack( ( np.array(df['search_term']), np.array(df['product_description'].apply(en)) ))[0]\n",
    "    title_zip = np.dstack( (np.array(df['search_term']), np.array(df['product_title'].apply(en)) ))[0]\n",
    "    attr_zip = np.dstack( (np.array(df['search_term']), np.array(df['attr'].apply(en)) ))[0]\n",
    "    brand_zip = np.dstack( (np.array(df['search_term']), np.array(df['brand'].apply(en)) ))[0]\n",
    "    \n",
    "    \n",
    "    # Number of unique words that are matched to the comparison string (float)\n",
    "    matched_strings = [\n",
    "        ['desc_matched_string', l_matched_words_string, desc_zip],\n",
    "        ['title_matched_string', l_matched_words_string, title_zip],\n",
    "        ['attr_matched_string', l_matched_words_string, attr_zip],\n",
    "        ['brand_matched_string', l_matched_words_string, brand_zip]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Total number of query words matched to the comparison string (float)\n",
    "    count_string = [\n",
    "        ['desc_count_string', l_count_matched_words_string, desc_zip],\n",
    "        ['title_count_string', l_count_matched_words_string, title_zip],\n",
    "        ['attr_count_string', l_count_matched_words_string, attr_zip]\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Whether or not the whole query (string) can be found in the comparison string (bool)\n",
    "    query_matched = [\n",
    "        ['desc_query_matched', l_matched_query, desc_zip],\n",
    "        ['title_query_matched', l_matched_query, title_zip],\n",
    "        ['attr_query_matched', l_matched_query, attr_zip]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # How many times the query is in the comparison string (float)\n",
    "    query_count = [\n",
    "        ['desc_query_count', l_count_matched_query, desc_zip],\n",
    "        ['title_query_count', l_count_matched_query, title_zip],\n",
    "        ['attr_query_count', l_count_matched_query, attr_zip]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # Is the last word of the query in the comparison string? (bool)\n",
    "    last_word = [\n",
    "        ['desc_last_word', l_last_word, desc_zip],\n",
    "        ['title_last_word', l_last_word, title_zip]\n",
    "    ]\n",
    "    \n",
    "    # Is the first word of the query in the comparison string? (bool)\n",
    "    first_word = [\n",
    "        ['desc_first_word', l_first_word, desc_zip],\n",
    "        ['title_first_word', l_first_word, title_zip]\n",
    "    ]\n",
    "    \n",
    "    ## Add the columns\n",
    "    cols_to_add = [char_lengths, word_lengths]\n",
    "    for cols in cols_to_add:\n",
    "        for col in cols:\n",
    "            df[col[0]] = run_pool(col[1], col[2])\n",
    "    \n",
    "    # Fraction of unique words matched divided by number of unique words in the query (float)\n",
    "    #df['desc_frac_matched'] = df['desc_matched_string'] / df['query_word_l']\n",
    "    #df['title_frac_matched'] = df['title_matched_string'] / df['query_word_l']\n",
    "    #df['attr_frac_matched_query'] = df['attr_matched_string'] / df['query_word_l']\n",
    "    # Also by the attr length\n",
    "    #df['attr_frac_matched_attr'] = df['attr_matched_string'] / df['attr_word_l']\n",
    "    \n",
    "    \n",
    "    # Drop NaNs\n",
    "    copy_df = copy.deepcopy(df.dropna())\n",
    "    \n",
    "    print display(HTML(\"<font color='blue'><b>Data pipelined in %s s</b></font>\"%(time.time()-start)))\n",
    "    return copy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font color='blue'><b>Data pipelined in 23.9278800488 s</b></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "X_train = feature_pipeline(f_train)\n",
    "y_train = X_train['relevance'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "      <th>attr</th>\n",
       "      <th>brand</th>\n",
       "      <th>desc_char_l</th>\n",
       "      <th>title_char_l</th>\n",
       "      <th>query_char_l</th>\n",
       "      <th>attr_char_l</th>\n",
       "      <th>brand_char_l</th>\n",
       "      <th>desc_word_l</th>\n",
       "      <th>title_word_l</th>\n",
       "      <th>query_word_l</th>\n",
       "      <th>attr_word_l</th>\n",
       "      <th>brand_word_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>Versatile connector for various 90° connection...</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "      <td>847</td>\n",
       "      <td>33</td>\n",
       "      <td>13</td>\n",
       "      <td>413</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>l bracket</td>\n",
       "      <td>2.50</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>Versatile connector for various 90° connection...</td>\n",
       "      <td>Simpson Strong-Tie</td>\n",
       "      <td>847</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>413</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>100002</td>\n",
       "      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>3.00</td>\n",
       "      <td>BEHR Premium Textured DECKOVER is an innovativ...</td>\n",
       "      <td>Brush,Roller,Spray 6.63 in 7.76 in 6.63 in Rev...</td>\n",
       "      <td>BEHR Premium Textured DeckOver</td>\n",
       "      <td>1102</td>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>905</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>100005</td>\n",
       "      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n",
       "      <td>rain shower head</td>\n",
       "      <td>2.33</td>\n",
       "      <td>Update your bathroom with the Delta Vero Singl...</td>\n",
       "      <td>Combo Tub and Shower No Includes the trim kit ...</td>\n",
       "      <td>Delta</td>\n",
       "      <td>694</td>\n",
       "      <td>78</td>\n",
       "      <td>16</td>\n",
       "      <td>616</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                                      product_title  \\\n",
       "0   2       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   3       100001                  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   9       100002  BEHR Premium Textured DeckOver 1-gal. #SC-141 ...   \n",
       "3  16       100005  Delta Vero 1-Handle Shower Only Faucet Trim Ki...   \n",
       "\n",
       "        search_term  relevance  \\\n",
       "0     angle bracket       3.00   \n",
       "1         l bracket       2.50   \n",
       "2         deck over       3.00   \n",
       "3  rain shower head       2.33   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Not only do angles make joints stronger, they ...   \n",
       "1  Not only do angles make joints stronger, they ...   \n",
       "2  BEHR Premium Textured DECKOVER is an innovativ...   \n",
       "3  Update your bathroom with the Delta Vero Singl...   \n",
       "\n",
       "                                                attr  \\\n",
       "0  Versatile connector for various 90° connection...   \n",
       "1  Versatile connector for various 90° connection...   \n",
       "2  Brush,Roller,Spray 6.63 in 7.76 in 6.63 in Rev...   \n",
       "3  Combo Tub and Shower No Includes the trim kit ...   \n",
       "\n",
       "                            brand  desc_char_l  title_char_l  query_char_l  \\\n",
       "0              Simpson Strong-Tie          847            33            13   \n",
       "1              Simpson Strong-Tie          847            33             9   \n",
       "2  BEHR Premium Textured DeckOver         1102            79             9   \n",
       "3                           Delta          694            78            16   \n",
       "\n",
       "   attr_char_l  brand_char_l  desc_word_l  title_word_l  query_word_l  \\\n",
       "0          413            18            1             1             1   \n",
       "1          413            18            1             1             1   \n",
       "2          905            30            1             1             1   \n",
       "3          616             5            1             1             1   \n",
       "\n",
       "   attr_word_l  brand_word_l  \n",
       "0            1             1  \n",
       "1            1             1  \n",
       "2            1             1  \n",
       "3            1             1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Plot the Feature Distributions\n",
    "As a sanity check, it is good to check out the first few lines of my data frame and also to graph the features to make sure there are actual distributions of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution (histogram) of my features\n",
    "def plot_hist(col, name):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    plt.title('%s' %name, fontsize=15)\n",
    "    #fig.colorbar(cax)\n",
    "    plt.hist(col)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_feature_plots(df):\n",
    "    # Feature plots\n",
    "    skip_cols = ['id', 'product_uid', 'relevance','search_term', 'brand',\n",
    "                     'product_title','product_description','attr']\n",
    "    \n",
    "    features = list(df.drop(skip_cols, axis=1).columns.values)\n",
    "    # Plot a bunch of stuff\n",
    "    dim = len(features)/3 + 1 if len(features)%3 > 0 else len(features)/3\n",
    "\n",
    "    f, axarr = plt.subplots(dim, 3, figsize=(16,20))\n",
    "    plt.tight_layout(pad=3)\n",
    "    \n",
    "    for i in range(0, dim):\n",
    "        # For each row\n",
    "        for j in range(0, 3):\n",
    "            # For each element in the row\n",
    "            if (i*3 + j) < len(features):\n",
    "                # As long as the chart exists in the tuple\n",
    "                axarr[i][j].hist( df[ features[i*3+j] ], color='orange' )\n",
    "                axarr[i][j].set_title( features[i*3+j], fontsize=15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run_feature_plots(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Learning\n",
    "A few notes about the distributions:\n",
    "\n",
    "* The string length columns look to be distributed pretty nicely\n",
    "* The description matches are heavily favored to the right (meaning the strings match well); we would expect this from a search engine\n",
    "* The relevance scores are also heavily favored to the right (again, we expect this engine to work reasonably well, so this makes makes sense)\n",
    "\n",
    "Everything so far looks reasonable. Now I will go ahead and set up a machine learning pipeline to test some algorithms on the training/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pipeline Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Define my custom pipeline\n",
    "class CustomPipeline(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        drop_cols = ['id', 'product_uid', 'relevance','search_term', 'brand',\n",
    "                     'product_title','product_description','attr']\n",
    "        new_df = df.drop(drop_cols, axis=1).values\n",
    "        return new_df\n",
    "    \n",
    "# This is a \n",
    "class TextPipeline(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data_dict):\n",
    "        # Convert dict into a string\n",
    "        return data_dict[self.key].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor as BR\n",
    "from sklearn import pipeline, grid_search\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "## Use a random forest\n",
    "rfr = RandomForestRegressor(n_jobs=-1, n_estimators=400, max_depth=15, verbose=0)\n",
    "\n",
    "# Use tf-idf sk-learn functions to vectorize the documents\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "# After tf-idf, reduce dimensionality of the vector\n",
    "tsvd = TruncatedSVD(n_components=10)\n",
    "\n",
    "## Define the pipeline\n",
    "_pipeline = pipeline.Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('cst',  CustomPipeline()),\n",
    "            ('txt1', pipeline.Pipeline([\n",
    "                ('s1', TextPipeline(key='search_term')),\n",
    "                ('tfidf1', tfidf), \n",
    "                ('tsvd1', tsvd)\n",
    "            ])),\n",
    "            ('txt2', pipeline.Pipeline([\n",
    "                ('s2', TextPipeline(key='product_title')), \n",
    "                ('tfidf2', tfidf),\n",
    "                ('tsvd2', tsvd)\n",
    "            ])),\n",
    "            #('txt3', pipeline.Pipeline([('s3', TextPipeline(key='product_description')), ('tfidf3', tfidf), ('tsvd3', tsvd)])),\n",
    "            ('txt4', pipeline.Pipeline([\n",
    "                ('s4', TextPipeline(key='brand')),\n",
    "                ('tfidf4', tfidf), \n",
    "                ('tsvd4', tsvd)\n",
    "            ]))        \n",
    "        ],\n",
    "        transformer_weights = {\n",
    "            'cst': 1.0,\n",
    "            'txt1': 0.5,\n",
    "            'txt2': 0.25,\n",
    "            #'txt3': 0.05,\n",
    "            'txt4': 0.5\n",
    "        },\n",
    "        #n_jobs = -1\n",
    "    )), \n",
    "    ('rfr', rfr)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# Define the loss function; this is a custom root-MSE (RMSE) function with tighter errors\n",
    "def f_mse(y, y_pred):\n",
    "    return mean_squared_error(y, y_pred)**0.5\n",
    "RMSE = make_scorer(f_mse, greater_is_better=False)\n",
    "\n",
    "# Param grid for GridSearch\n",
    "param_grid = { 'rfr__max_features': [10],'rfr__max_depth': [20] }\n",
    "\n",
    "# Arguments for GridSearch\n",
    "grid_search_args = {\n",
    "    'estimator': _pipeline,\n",
    "    'param_grid': param_grid,\n",
    "    #'n_jobs': -1,\n",
    "    'cv': 4,\n",
    "    'verbose': 0,\n",
    "    'scoring': RMSE\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = grid_search.GridSearchCV(**grid_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN GridSearch\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print \"GridSearchCV completed in %s s\"%(time.time()-start)\n",
    "print \"Best parameters found by grid search: %s\"%model.best_params_\n",
    "print \"Best CV score: %s\"%model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Test Set\n",
    "Now I will move over to the test set. I will predict based on the model I just generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test = feature_pipeline(f_test)\n",
    "\n",
    "# Separate the ids\n",
    "df_test_ids = df_test['id']\n",
    "\n",
    "# Need to add this column temporarily; it will get dropped in the pipeline\n",
    "df_test['relevance'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Predict test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict!\n",
    "test_y = model.predict(df_test)\n",
    "final_y = map(lambda x: 1 if x < 1. else 3 if x > 3. else x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Look at distributions\n",
    "After looking at the data, I want to look at the distribution and compare it to the one from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(final_y, 'Scores in Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(train_y, 'Normalized Relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Test mean: %s, std: %s\"%(np.mean(final_y), np.std(final_y))\n",
    "print \"Training mean: %s, std: %s\"%(np.mean(train_y), np.std(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Submission\n",
    "Now I am finally ready to write the submission file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.shape(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"submission4\"\n",
    "\n",
    "path = \"%s/%s.csv\"%(DATADIR, file_name)\n",
    "pd.DataFrame({\"id\": df_test_ids.apply(int), \"relevance\": test_y}).to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
