{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot Product Search Relevance\n",
    "The goal of this analysis is to determine how to predict relevance of a search on Home Depot's website. The training data were labelled by crowdsourcing humans, but the hope is that the text and numerical features will be enough to predict relevance via machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import my library stack\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import os\n",
    "import pprint\n",
    "import copy\n",
    "import gc\n",
    "import re\n",
    "%matplotlib inline\n",
    "\n",
    "# Some nice display tools for ipython\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# There are several files that the Kaggle competition included for this analysis\n",
    "DATADIR = \"%s/home_depot_2015/\"%os.environ[\"KAGGLE_DATA_DIR\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Overview of the data\n",
    "There are three files I will take a peek at here:\n",
    "    \n",
    "* **train.csv** -- The training set, which contains products, searches, and relevance scores\n",
    "* **test.csv** -- The test set, which contains products and searches --> I am to predict relevance scores\n",
    "    \n",
    "* **product_descriptions.csv** -- Contains product id and a plain text description of the product\n",
    "    \n",
    "* **attributes.csv** -- Contains product id and several attributes, but for only a *subset* of products\n",
    "\n",
    "I'm just going to preview the first few rows of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>First three rows of train</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>angle bracket</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                      product_title    search_term  \\\n",
       "0   2       100001  Simpson Strong-Tie 12-Gauge Angle  angle bracket   \n",
       "\n",
       "   relevance  \n",
       "0          3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>First three rows of test</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>90 degree bracket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                      product_title        search_term\n",
       "0   1       100001  Simpson Strong-Tie 12-Gauge Angle  90 degree bracket"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>First three rows of descriptions</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                product_description\n",
       "0       100001  Not only do angles make joints stronger, they ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>First three rows of attributes</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>Bullet01</td>\n",
       "      <td>Versatile connector for various 90° connection...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid      name                                              value\n",
       "0       100001  Bullet01  Versatile connector for various 90° connection..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first 5 rows of a csv file given the path to it\n",
    "def preview_data(file, name):\n",
    "    print display(HTML(\"<h3>First three rows of %s</h3>\"%name))\n",
    "    preview_df = pd.read_csv(file)\n",
    "    print display(preview_df.head(1))\n",
    "\n",
    "def get_path(file):\n",
    "    return \"%s%s\"%(DATADIR, file)\n",
    "\n",
    "# Define the files for later\n",
    "f_train = get_path('train.csv')\n",
    "f_test = get_path('test.csv')\n",
    "f_desc = get_path('product_descriptions.csv')\n",
    "f_attr = get_path('attributes.csv')\n",
    "\n",
    "# Do all four\n",
    "files = [(f_train, 'train'), (f_test, 'test'), (f_desc, 'descriptions'), (f_attr, 'attributes')]\n",
    "map(lambda x: preview_data(x[0], x[1]), files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Feature engineering\n",
    "After looking at these spreadsheets, I realize there isn't a ton of information with which to work. My initial thought is to do some sort of a word matching procedure (e.g. see if one of the search words matches one of the words in the title (or description, or attributes). Better still, I could take the individual letters in each word of the search query and try to see if they appear consecutively in the raw string of the title, description, or attributes.\n",
    "\n",
    "Let's give that a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I will definitely want to use multiprocessing in this one\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The attributes are a little trickier. There may be 0 or many attributes per 1 product_uid\n",
    "# I want to concatenate all strings belonging to a particular product_uid\n",
    "def collapse_attr(data):\n",
    "    attr = {}\n",
    "    for d in data:\n",
    "        # d is an array of form [product_uid, name, value]\n",
    "        if not np.isnan(d[0]):\n",
    "            # Concatenate the attribute if it exists, otherwise add it\n",
    "            if str(int(d[0])) in attr:\n",
    "                attr[str(int(d[0]))] = \"%s %s\"%( attr[str(int(d[0]))], str(d[2]) )\n",
    "            else:\n",
    "                attr[str(int(d[0]))] = str(d[2])\n",
    "\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the training attribute array, which we will append to the dataframe in the pipeline\n",
    "ATTR_ARR = collapse_attr(np.array(pd.read_csv(f_attr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for processing the strings\n",
    "These will format the strings, add alternate suffixes, and add some common abbreviations if applicable. Since we're dealing with Home Depot data, we have a general idea of what types of abbreviations we might encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a word, return all forms of it and its abbreviations\n",
    "def abbrev(s):\n",
    "    abrv_groups = [\n",
    "        [\"'\", \"in\", \"inches\", \"inch\"],\n",
    "        [\"pounds\", \"pound\", \"lbs\", \"lb\"],\n",
    "        [\"sqft\", \"sq\", \"sf\", \"square\", \"squared\", \"foot\", \"feet\", \"\\\"\", \"ft\", \"inch\", \"inches\", \"in\", \"'\"],\n",
    "        [\"cf\", \"cu\", \"cubic\", \"cubed\", \"inch\", \"foot\", \"feet\", \"'\", \"\\\"\", \"ft\", \"in\", \"inches\"],\n",
    "        [\"gal\", \"gallon\", \"gallons\"],\n",
    "        [\"g\", \"gram\", \"grams\", \"kg\", \"kilogram\", \"kilo\"],\n",
    "        [\"oz\", \"ounces\", \"ounce\"],\n",
    "        [\"cm\", \"centimeters\", \"centimeter\"],\n",
    "        [\"m\", \"meter\", \"meters\"],\n",
    "        [\"mm\", \"milimeter\", \"millimeter\", \"milimeters\", \"millimeters\"],\n",
    "        [\"a\", \"amp\", \"amps\", \"ampere\", \"amperes\"],\n",
    "        [\"w\", \"watt\", \"watts\"],\n",
    "        [\"v\", \"volt\", \"volts\"],\n",
    "        [\"whirpool\",\"whirlpool\", \"whirlpoolga\", \"whirlpoolstainless\",\"stainless\"],\n",
    "        [\"and\", \"&\", \"+\", \"&amp;\"],\n",
    "        [\"x\", \"by\", \"*\"],\n",
    "        [\"deg\", \"degree\", \"degrees\", \"°\", \"angle\"],\n",
    "        ['dia', 'diameter']\n",
    "    ]\n",
    "    \n",
    "    # If we can match the word in an abbreviation group, return the whole group\n",
    "    for g in abrv_groups:\n",
    "        if s in g:\n",
    "            return g\n",
    "        \n",
    "    # If we can't match anything just return an empty array\n",
    "    return []\n",
    "\n",
    "# Turn the string into a series of words\n",
    "def process_string(s):\n",
    "    \n",
    "    __words = np.array(s.split(\" \"))\n",
    "    \n",
    "    # Split by special, but include those characters\n",
    "    # This regex splits by the characters [', \", /, *, -], but INCLUDES those characters\n",
    "    _words = list(np.hstack(map(lambda i: re.split(r\"(plus|[('\\\"\\-*/)])\", i), __words)))\n",
    "\n",
    "    # I want the original words plus the split words\n",
    "    # e.g. \"some 3x3\" should now be [\"some\", \"3-3\", \"3\", \"-\", \"3\"]\n",
    "    #extra_words = filter(lambda x: x!=None, map(lambda x: x if \\\n",
    "    #                    (\"'\" in x or \"-\" in x or \"-\" in x or \"\\\"\" in x or \"*\" in x) else None, __words))\n",
    "    #words = np.hstack([extra_words, _words])\n",
    "    words = _words\n",
    "    \n",
    "    # Get rid of commas\n",
    "    words = map(lambda x: x.replace(',', ''), words)\n",
    "    # Get rid of semicolons\n",
    "    words = map(lambda x: x.replace(';', ''), words)\n",
    "    # Get rid of colons\n",
    "    words = map(lambda x: x.replace(':', ''), words)\n",
    "    # Get rid of periods\n",
    "    words = map(lambda x: x.replace('.', ''), words)\n",
    "    # Get rid of blanks\n",
    "    words = filter(lambda x: x != ' ' and x != '', words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def pre_process_strings(query):\n",
    "    \n",
    "    # Lowercase all the things\n",
    "    query = str(query.lower())\n",
    "    \n",
    "    # Split the query into an array of char arrays\n",
    "    query_words = process_string(query)\n",
    "\n",
    "    return query_words\n",
    "\n",
    "\n",
    "# This function processes strings like \"4x4\" or \"4'x4'\"\n",
    "def process_x_by(s):\n",
    "    if any(i.isdigit() for i in s) and \"x\" in s:\n",
    "        new_s = list(filter(lambda x: x!='', s.split(\"x\"))); new_s.append(s); new_s.append(\"x\")\n",
    "        return new_s\n",
    "    else:\n",
    "        return [s]\n",
    "\n",
    "# Split strings that have unit suffixes (e.g. 10in, 50g, etc)\n",
    "def process_unit_suffixes(s):\n",
    "    strings = [\"mm\", \"cm\", \"m\", \"g\", \"kg\", \"in\", \"ft\", \"a\", \"w\", \"v\", \"oz\", \"gal\", \"cf\"]\n",
    "    # Inefficient but I can't think of a better way\n",
    "    for i in strings:\n",
    "        # If any of the above strings is in s, return that string + the number\n",
    "        if i in s and any(j.isdigit() for j in s):\n",
    "            arr = list(filter(lambda x: x!='', s.split(i))); arr.append(i); arr.append(s)\n",
    "            return arr\n",
    "\n",
    "    return [s]\n",
    "\n",
    "# Get a list of words similar to the word if applicable\n",
    "# This will get called with a word in the QUERY\n",
    "def extension_words(word):\n",
    "    \n",
    "    # Make damn sure everything is lower case\n",
    "    w = word.lower()\n",
    "    \n",
    "    # Process strings of the form \"AxB\"\n",
    "    ret_words = process_x_by(w)\n",
    "    \n",
    "    # Include abbreviation words\n",
    "    abbr = abbrev(w)\n",
    "    \n",
    "    # Flatten array\n",
    "    ret_words = list(np.hstack([ret_words, abbr]))\n",
    "    \n",
    "    \n",
    "    # If the word is small (<4 chars), contains a number, or contains a special character,\n",
    "    #     only add s and return\n",
    "    if any(i.isdigit() for i in w) or len(list(word)) < 4 or any(i in w for i in [\"-\", \"*\", \"'\", \"\\\"\", \"/\"]):\n",
    "        ret_words.append(\"%ss\"%w)\n",
    "        return filter(lambda x: x!='' and x!=' ', ret_words)\n",
    "\n",
    "    \n",
    "    # A list of suffixes\n",
    "    suffixes = ['s', 'ed', 'ing', 'n', 'en', 'er', 'est', 'ise', 'fy', 'ly',\n",
    "               'ful', 'able', 'ible', 'hood', 'ess', 'ness', 'less', 'ism',\n",
    "               'ment', 'ist', 'al', 'ish', 'tion']\n",
    "    \n",
    "    # If the word ends in one of these suffixes, add the smaller version\n",
    "    # to strings; otherwise, add this to the end of the word and add that\n",
    "    for x in xrange(len(suffixes)):\n",
    "        l = len(suffixes[x])\n",
    "        if w[-l:] == suffixes[x]:\n",
    "            ret_words.append(w[0:-l])\n",
    "        else:\n",
    "            ret_words.append(w+suffixes[x])\n",
    "\n",
    "    return filter(lambda x: x!='' and x!=' ', ret_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_df = pd.read_csv(f_train)\n",
    "#query_words = filter(lambda x: x!=None, _df['search_term'].apply(lambda x: pre_process_strings(x) if \"-\" in x else None))\n",
    "\n",
    "#for i in xrange(100, 200):\n",
    "#    print \"query words: %s\\ntitle: %s\\nrating: %s\"%(_df['search_term'][i], _df['product_title'][i], _df['relevance'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for doing word searches\n",
    "These will determine if words in the query are in the matching string. We want to know three basic things:\n",
    "* Does the string contain *any* of the query words?\n",
    "* What fraction of the query words are in the matching words?\n",
    "* What fraction of the chars making up query words are found in the matching words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine if the word is in the comparison string\n",
    "#   @returns 1 or 0\n",
    "#--------------------------------------------------------\n",
    "def match_word(word, compare):\n",
    "    \n",
    "    #strings = filter( lambda x: x!='' and x!=' ', extension_words(word) )\n",
    "    strings = extension_words(word)\n",
    "\n",
    "    # If no words are large enough, return a 0 match count\n",
    "    if not strings: return 0\n",
    "    \n",
    "    # Return the number of words matched\n",
    "    return min(1, sum( map(lambda s: 1 if s in compare else 0, strings ) ))\n",
    "\n",
    "# Determine the number of times the word (or any version of it) matches a string\n",
    "#   @returns array of match counts\n",
    "def match_word_count(word, compare):\n",
    "    \n",
    "    strings = filter( lambda x: x!='' and x!=' ', extension_words(word) )\n",
    "\n",
    "    # If no words are large enough, return a 0 match count\n",
    "    if not strings: return 0\n",
    "    \n",
    "    # Return the number of times any of the words were matched\n",
    "    return max( map(lambda s: compare.count(s) , strings ) )\n",
    "    \n",
    "    \n",
    "    \n",
    "## STRING MATCHING FUNCTIONS\n",
    "##=========================================================\n",
    "\n",
    "## Word matching to the original string\n",
    "#----------------------------------------------------------------------\n",
    "# Get the number of unique words that are matched\n",
    "def matched_words_string(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    return sum( map(lambda x: match_word(x, to_match.lower()), query_words) ) if query_words else 0\n",
    "    \n",
    "    \n",
    "# Get the count of all words matched (i.e. if a word is matched more than once, it is counted multiple times)\n",
    "def count_matched_words_string(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    return sum( map(lambda x: match_word_count(x, to_match.lower()), query_words) )\n",
    "\n",
    "## Word matching to the words in the string\n",
    "def matched_words_words(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    to_match_words = to_match.split(\" \")\n",
    "    sums = map(lambda y: sum( map(lambda x: match_word(x, to_match.lower()), query_words) ), \\\n",
    "                to_match_words) if query_words else 0\n",
    "    return sum(sums)\n",
    "\n",
    "# Count of words matching the words in the string\n",
    "def count_matched_words_words(query, to_match):\n",
    "    query_words = pre_process_strings(query)\n",
    "    to_match_words = to_match.split(\" \")\n",
    "    counts = map(lambda y: sum( map(lambda x: match_word_count(x, to_match.lower()), query_words) ), \\\n",
    "                to_match_words) if query_words else 0\n",
    "    return sum(counts)\n",
    "        \n",
    "\n",
    "\n",
    "## QUERY matching\n",
    "#----------------------------------------------------------------------\n",
    "# Whether or not the whole query is in the string\n",
    "def matched_query(query, to_match):\n",
    "    return query in to_match\n",
    "\n",
    "# How many times the whole query is in the string\n",
    "def count_matched_query(query, to_match):\n",
    "    return to_match.count(query)\n",
    "\n",
    "# Whether or not the last word of the query is in the to_match string\n",
    "def last_word(query, to_match):\n",
    "    last_q = query.split(\" \")[-1]\n",
    "    return 1 if last_q in to_match else 0\n",
    "\n",
    "# Whether or not a word from the query is the FIRST word in the to_match string\n",
    "def first_word(query, to_match):\n",
    "    first_q = query.split(\" \")[0]\n",
    "    return 1 if first_q in to_match else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Feature Engineering Pipeline\n",
    "I will start by engineering new features and removing the long strings in my data set. Specifically, I want to add\n",
    "\n",
    "* Match rates of query relating to title and description (determined by char_match_fraction function)\n",
    "* String length columns of query, description, and title columns\n",
    "\n",
    "I will go ahead and build a new training set based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# POOL lambda functions (need to be defined outside the function that calls them)\n",
    "# With multiprocessing, we can't use lambda, so I will define some basic functions here\n",
    "#================================================================================\n",
    "def lambda_in_attr(a):\n",
    "    return ATTR_ARR[str(int(a))] if str(int(a)) in ATTR_ARR else ''\n",
    "\n",
    "# Lengths\n",
    "def lambda_char_len(a):\n",
    "    return float(len(list(str(a))))\n",
    "def lambda_word_len(a):\n",
    "    return float(len(str(a).split(\" \")))\n",
    "\n",
    "# Matched words\n",
    "def l_matched_words_string(a):\n",
    "    return float(matched_words_string( str(a[0]), str(a[1]) ))\n",
    "def l_count_matched_words_string(a):\n",
    "    return float(count_matched_words_string( str(a[0]), str(a[1]) ))\n",
    "def l_matched_words_words(a):\n",
    "    return float(matched_words_words( str(a[0]), str(a[1]) ))\n",
    "def l_count_matched_words_words(a):\n",
    "    return float(count_matched_words_words( str(a[0]), str(a[1]) ))\n",
    "\n",
    "# Matched queries\n",
    "def l_matched_query(a):\n",
    "    return matched_query( str(a[0]), str(a[1]) )\n",
    "def l_count_matched_query(a):\n",
    "    return float(count_matched_query( str(a[0]), str(a[1]) ))\n",
    "\n",
    "# Binaries\n",
    "def l_last_word(a):\n",
    "    return last_word( str(a[0]), str(a[1]) )\n",
    "def l_first_word(a):\n",
    "    return first_word( str(a[0]), str(a[1]) )\n",
    "\n",
    "\n",
    "def unit(a):\n",
    "    return a\n",
    "    #return a / a.max()\n",
    "\n",
    "#================================================================================\n",
    "## DATA PIPELINE\n",
    "#================================================================================\n",
    "# Given the data (train or test) and description files,\n",
    "# perform a series of operations to produce a data set on which we can do ML\n",
    "def feature_pipeline(data_file, **kwargs):\n",
    "    \n",
    "    # Define my multiprocessing pool and start the timer\n",
    "    POOL = Pool(maxtasksperchild=1000)\n",
    "    start = time.time()\n",
    "    \n",
    "    #============\n",
    "    # Read files\n",
    "    #============\n",
    "    \n",
    "    # Read the initial train.csv and join it to product descriptions\n",
    "    _df = pd.read_csv(data_file)\n",
    "\n",
    "    # Add in descriptions because they are 1:1\n",
    "    df = pd.merge(_df, pd.read_csv(f_desc), how='outer')\n",
    "    \n",
    "    # If there is an attribute for a product uid, join it\n",
    "    df['attr'] = POOL.map(lambda_in_attr, df['product_uid'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #===========================================\n",
    "    # ADD LENGTH Columns (filter out whitespace)\n",
    "    #===========================================\n",
    "\n",
    "    # Char lengths\n",
    "    df['desc_char_l'] = unit(pd.Series( POOL.map(lambda_char_len, df['product_description']) ))\n",
    "    df['title_char_l'] = unit(pd.Series( POOL.map(lambda_char_len, df['product_title']) ))\n",
    "    df['query_char_l'] =  unit(pd.Series( POOL.map(lambda_char_len, df['search_term']) ))\n",
    "    df['attr_char_l'] = pd.Series( POOL.map(lambda_char_len, df['attr']) )\n",
    "    \n",
    "    # Word lengths\n",
    "    df['desc_word_l'] = unit(pd.Series( POOL.map(lambda_word_len, df['product_description']) ))\n",
    "    df['title_word_l'] = unit(pd.Series( POOL.map(lambda_word_len, df['product_title']) ))\n",
    "    df['query_word_l'] = unit(pd.Series( POOL.map(lambda_word_len, df['search_term']) ))\n",
    "    df['attr_word_l'] = pd.Series( POOL.map(lambda_word_len, df['attr']) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    #====================\n",
    "    # ADD MATCH COLUMNS\n",
    "    #====================\n",
    "    \n",
    "    # Zip the data into tuples\n",
    "    desc_zip = np.dstack( ( np.array(df['search_term']), np.array(df['product_description']) ))[0]\n",
    "    title_zip = np.dstack( (np.array(df['search_term']), np.array(df['product_title']) ))[0]\n",
    "    attr_zip = np.dstack( (np.array(df['search_term']), np.array(df['attr']) ))[0]\n",
    "    \n",
    "    \n",
    "    # Number of unique words that are matched to the comparison string (float)\n",
    "    df['desc_matched_string'] = unit(pd.Series( POOL.map(l_matched_words_string, desc_zip ) ))\n",
    "    df['title_matched_string'] = unit(pd.Series( POOL.map(l_matched_words_string, title_zip ) ))\n",
    "    df['attr_matched_string'] = unit(pd.Series( POOL.map(l_matched_words_string, attr_zip ) ))\n",
    "    \n",
    "    \n",
    "    # Total number of query words matched to the comparison string (float)\n",
    "    df['desc_count_string'] = unit(pd.Series( POOL.map(l_count_matched_words_string, desc_zip ) ))\n",
    "    df['title_count_string'] = unit(pd.Series( POOL.map(l_count_matched_words_string, title_zip ) ))\n",
    "    df['attr_count_string'] = unit(pd.Series( POOL.map(l_count_matched_words_string, attr_zip ) ))\n",
    "    \n",
    "    \n",
    "    # Whether or not the whole query (string) can be found in the comparison string (bool)\n",
    "    df['desc_query_matched'] = pd.Series( POOL.map(l_matched_query, desc_zip ) )\n",
    "    df['title_query_matched'] = pd.Series( POOL.map(l_matched_query, title_zip ) )\n",
    "    df['attr_query_matched'] = pd.Series( POOL.map(l_matched_query, attr_zip ) )\n",
    "    \n",
    "    \n",
    "    # How many times the query is in the comparison string (float)\n",
    "    df['desc_query_count'] = unit(pd.Series( POOL.map(l_count_matched_query, desc_zip ) ))\n",
    "    df['title_query_count'] = unit(pd.Series( POOL.map(l_count_matched_query, title_zip ) ))\n",
    "    df['attr_query_count'] = unit(pd.Series( POOL.map(l_count_matched_query, attr_zip ) ))\n",
    "    \n",
    "    \n",
    "    # Fraction of unique words matched divided by number of unique words in the query (float)\n",
    "    df['desc_frac_matched'] = unit(df['desc_matched_string'] / df['query_word_l'])\n",
    "    df['title_frac_matched'] = unit(df['title_matched_string'] / df['query_word_l'])\n",
    "    df['attr_frac_matched_query'] = unit(df['attr_matched_string'] / df['query_word_l'])\n",
    "    # Also by the attr length\n",
    "    df['attr_frac_matched_attr'] = unit(df['attr_matched_string'] / df['attr_word_l'])\n",
    "    \n",
    "    \n",
    "    # Is the last word of the query in the comparison string? (bool)\n",
    "    df['desc_last_word'] = pd.Series( POOL.map(l_last_word, desc_zip) )\n",
    "    df['title_last_word'] = pd.Series( POOL.map(l_last_word, title_zip) )\n",
    "    \n",
    "    \n",
    "    # Is the first word of the query in the comparison string? (bool)\n",
    "    df['desc_first_word'] = pd.Series( POOL.map(l_first_word, desc_zip) )\n",
    "    df['title_first_word'] = pd.Series( POOL.map(l_first_word, title_zip) )\n",
    "    \n",
    "    \n",
    "    #df['desc_matched_words'] = pd.Series( POOL.map(l_matched_words_words, desc_zip ) )\n",
    "    #df['desc_count_words'] = pd.Series( POOL.map(l_count_matched_words_words, desc_zip ) )\n",
    "    #df['title_matched_words'] = pd.Series( POOL.map(l_matched_words_words, title_zip ) )\n",
    "    #df['title_count_words'] = pd.Series( POOL.map(l_count_matched_words_words, title_zip ) )\n",
    "    #df['attr_matched_words'] = pd.Series( POOL.map(l_matched_words_words, attr_zip ) )\n",
    "    #df['attr_count_words'] = pd.Series( POOL.map(l_count_matched_words_words, attr_zip ) )\n",
    "    \n",
    "    # Combo columns\n",
    "    #df['desc_char_word'] = df['desc_char'] * df['desc_word']\n",
    "    #df['title_char_word'] = df['title_char'] * df['title_word']\n",
    "    #df['desc_title_char'] = df['desc_char'] * df['title_char']\n",
    "    #df['desc_title_word'] = df['desc_word'] * df['title_word']\n",
    "    \n",
    "\n",
    "    \n",
    "    #======================\n",
    "    ## REMOVE TEXT columns\n",
    "    #======================\n",
    "    map(lambda x: df.pop(x), ['product_uid', 'search_term', 'product_title', 'product_description', 'attr'])\n",
    "\n",
    "    print \"df size: %s\"%str(np.shape(df))\n",
    "    ## DROP ROWS NaN values (but only if kwargs does not include submission)\n",
    "    if 'submission' in kwargs:\n",
    "        clean_df = df.copy()\n",
    "    else:\n",
    "        clean_df = df.copy().dropna()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #=========================================\n",
    "    ## POP OFF THE IDS and return the two dfs\n",
    "    #=========================================\n",
    "    ids = clean_df['id']\n",
    "    clean_df.pop('id')\n",
    "    print \"clean_df size: %s\"%str(np.shape(clean_df))\n",
    "    print display(HTML(\"<font color='blue'><b>Data pipelined in %s s</b></font>\"%(time.time()-start)))\n",
    "    \n",
    "    return clean_df, ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size: (143828, 26)\n",
      "clean_df size: (74067, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color='blue'><b>Data pipelined in 32.7238879204 s</b></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "df, ids = feature_pipeline(f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Plot the Feature Distributions\n",
    "As a sanity check, it is good to check out the first few lines of my data frame and also to graph the features to make sure there are actual distributions of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution (histogram) of my features\n",
    "def plot_hist(col, name):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    plt.title('%s' %name, fontsize=15)\n",
    "    #fig.colorbar(cax)\n",
    "    plt.hist(col)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def run_feature_plots():\n",
    "    # Feature plots\n",
    "    features = list(df.columns.values)\n",
    "    # Plot a bunch of stuff\n",
    "    dim = len(features)/3 + 1 if len(features)%3 > 0 else len(features)/3\n",
    "\n",
    "    f, axarr = plt.subplots(dim, 3, figsize=(16,20))\n",
    "    plt.tight_layout(pad=3)\n",
    "\n",
    "    for i in range(0, dim):\n",
    "        # For each row\n",
    "        for j in range(0, 3):\n",
    "            # For each element in the row\n",
    "            if (i*3 + j) < len(features):\n",
    "                # As long as the chart exists in the tuple\n",
    "                axarr[i][j].hist( df[ features[i*3+j] ], color='orange' )\n",
    "                axarr[i][j].set_title( features[i*3+j], fontsize=15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#run_feature_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Learning\n",
    "A few notes about the distributions:\n",
    "\n",
    "* The string length columns look to be distributed pretty nicely\n",
    "* The description matches are heavily favored to the right (meaning the strings match well); we would expect this from a search engine\n",
    "* The relevance scores are also heavily favored to the right (again, we expect this engine to work reasonably well, so this makes makes sense)\n",
    "\n",
    "Everything so far looks reasonable. Now I will go ahead and set up a machine learning pipeline to test some algorithms on the training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RF, BaggingRegressor as BR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Define X and y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the X and y matrices\n",
    "def define(df):\n",
    "    if 'relevance' in df:\n",
    "        y = pd.Series(df['relevance'])\n",
    "        df.pop('relevance')\n",
    "        X = np.array(df.copy())\n",
    "        return X, y\n",
    "    else:\n",
    "        print 'X and y already defined.'\n",
    "        return\n",
    "\n",
    "train_X, train_y = define(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Pipeline Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfr = RF(n_jobs=-1, n_estimators=400, max_depth=15)\n",
    "__models = pipeline.Pipeline([('RF',rfr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Setup Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "## Define the loss function\n",
    "# This is a custom root-MSE (RMSE) function with tighter errors\n",
    "def f_mse(y, y_pred):\n",
    "    return mean_squared_error(y, y_pred)**0.5\n",
    "\n",
    "RMSE = make_scorer(f_mse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'RF__max_features': [10]\n",
    "}\n",
    "\n",
    "grid_search_args = {\n",
    "    'estimator': __models,\n",
    "    'param_grid': param_grid,\n",
    "    'n_jobs': -1,\n",
    "    'cv': 5,\n",
    "    'verbose': 0,\n",
    "    'scoring': RMSE\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = grid_search.GridSearchCV(**grid_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Run Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV completed in 64.7383859158 s\n",
      "Best parameters found by grid search: {'RF__max_features': 10}\n",
      "Best CV score: -0.482872937601\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "model.fit(train_X, train_y)\n",
    "gc.collect()\n",
    "print \"GridSearchCV completed in %s s\"%(time.time()-start)\n",
    "print \"Best parameters found by grid search: %s\"%model.best_params_\n",
    "print \"Best CV score: %s\"%model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Test Set\n",
    "Now I will move over to the test set. I will predict based on the model I just generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test, df_test_ids = feature_pipeline(f_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Predict test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a new X and predict y using the model we fit earlier\n",
    "test_X = np.array(df_test.copy())\n",
    "test_y = model.predict(test_X)\n",
    "\n",
    "final_y = map(lambda x: 1 if x < 1. else 3 if x > 3. else x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Look at distributions\n",
    "After looking at the data, I want to look at the distribution and compare it to the one from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(final_y, 'Scores in Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(train_y, 'Normalized Relevance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Test mean: %s, std: %s\"%(np.mean(final_y), np.std(final_y))\n",
    "print \"Training mean: %s, std: %s\"%(np.mean(train_y), np.std(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Submission\n",
    "Now I am finally ready to write the submission file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Join ids with submission y\n",
    "def submit(ids, relevances, file_name):\n",
    "    \n",
    "    # ids need to be integers\n",
    "    ids = map(lambda x: int(x), ids)\n",
    "    \n",
    "    # Build a dataframe\n",
    "    submission = pd.DataFrame(index=ids)\n",
    "    submission.index.name = 'id'\n",
    "    submission['relevance'] = relevances\n",
    "    \n",
    "    # Print the head just for a sanity check\n",
    "    submission.head(10)\n",
    "    \n",
    "    # Write the file\n",
    "    path = \"%s/%s.csv\"%(DATADIR, file_name)\n",
    "    submission.to_csv(path, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit(df_test_ids, final_y, 'submission3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
